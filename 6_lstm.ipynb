{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8tQJd2YSCfWR"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D7tqLMoKF6uq"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 6\n",
    "------------\n",
    "\n",
    "After training a skip-gram model in `5_word2vec.ipynb`, the goal of this notebook is to train a LSTM character model over [Text8](http://mattmahoney.net/dc/textdata) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "MvEblsgEXxrd"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import tensorflow as tf\n",
    "import zipfile\n",
    "from six.moves import range\n",
    "from six.moves.urllib.request import urlretrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5993,
     "status": "ok",
     "timestamp": 1445965582896,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RJ-o3UBUFtCw",
    "outputId": "d530534e-0791-4a94-ca6d-1c8f1b908a9e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found and verified text8.zip\n"
     ]
    }
   ],
   "source": [
    "url = 'http://mattmahoney.net/dc/'\n",
    "\n",
    "def maybe_download(filename, expected_bytes):\n",
    "  \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n",
    "  if not os.path.exists(filename):\n",
    "    filename, _ = urlretrieve(url + filename, filename)\n",
    "  statinfo = os.stat(filename)\n",
    "  if statinfo.st_size == expected_bytes:\n",
    "    print('Found and verified %s' % filename)\n",
    "  else:\n",
    "    print(statinfo.st_size)\n",
    "    raise Exception(\n",
    "      'Failed to verify ' + filename + '. Can you get to it with a browser?')\n",
    "  return filename\n",
    "\n",
    "filename = maybe_download('text8.zip', 31344016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 5982,
     "status": "ok",
     "timestamp": 1445965582916,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "Mvf09fjugFU_",
    "outputId": "8f75db58-3862-404b-a0c3-799380597390"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data size 100000000\n"
     ]
    }
   ],
   "source": [
    "def read_data(filename):\n",
    "  with zipfile.ZipFile(filename) as f:\n",
    "    name = f.namelist()[0]\n",
    "    data = tf.compat.as_str(f.read(name))\n",
    "  return data\n",
    "  \n",
    "text = read_data(filename)\n",
    "print('Data size %d' % len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ga2CYACE-ghb"
   },
   "source": [
    "Create a small validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6184,
     "status": "ok",
     "timestamp": 1445965583138,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "w-oBpfFG-j43",
    "outputId": "bdb96002-d021-4379-f6de-a977924f0d02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99999000 ons anarchists advocate social relations based upon voluntary as\n",
      "1000  anarchism originated as a term of abuse first used against earl\n"
     ]
    }
   ],
   "source": [
    "valid_size = 1000\n",
    "valid_text = text[:valid_size]\n",
    "train_text = text[valid_size:]\n",
    "train_size = len(train_text)\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zdw6i4F8glpp"
   },
   "source": [
    "Utility functions to map characters to vocabulary IDs and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6276,
     "status": "ok",
     "timestamp": 1445965583249,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "gAL1EECXeZsD",
    "outputId": "88fc9032-feb9-45ff-a9a0-a26759cc1f2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected character: ï\n",
      "1 26 0 0\n",
      "a z  \n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = len(string.ascii_lowercase) + 1 # [a-z] + ' '\n",
    "first_letter = ord(string.ascii_lowercase[0])\n",
    "\n",
    "def char2id(char):\n",
    "  if char in string.ascii_lowercase:\n",
    "    return ord(char) - first_letter + 1\n",
    "  elif char == ' ':\n",
    "    return 0\n",
    "  else:\n",
    "    print('Unexpected character: %s' % char)\n",
    "    return 0\n",
    "  \n",
    "def id2char(dictid):\n",
    "  if dictid > 0:\n",
    "    return chr(dictid + first_letter - 1)\n",
    "  else:\n",
    "    return ' '\n",
    "\n",
    "print(char2id('a'), char2id('z'), char2id(' '), char2id('ï'))\n",
    "print(id2char(1), id2char(26), id2char(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lFwoyygOmWsL"
   },
   "source": [
    "Function to generate a training batch for the LSTM model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "collapsed": false,
    "executionInfo": {
     "elapsed": 6473,
     "status": "ok",
     "timestamp": 1445965583467,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "d9wMtjy5hCj9",
    "outputId": "3dd79c80-454a-4be0-8b71-4a4a357b3367"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ons anarchi', 'when milita', 'lleria arch', ' abbeys and', 'married urr', 'hel and ric', 'y and litur', 'ay opened f', 'tion from t', 'migration t', 'new york ot', 'he boeing s', 'e listed wi', 'eber has pr', 'o be made t', 'yer who rec', 'ore signifi', 'a fierce cr', ' two six ei', 'aristotle s', 'ity can be ', ' and intrac', 'tion of the', 'dy to pass ', 'f certain d', 'at it will ', 'e convince ', 'ent told hi', 'ampaign and', 'rver side s', 'ious texts ', 'o capitaliz', 'a duplicate', 'gh ann es d', 'ine january', 'ross zero t', 'cal theorie', 'ast instanc', ' dimensiona', 'most holy m', 't s support', 'u is still ', 'e oscillati', 'o eight sub', 'of italy la', 's the tower', 'klahoma pre', 'erprise lin', 'ws becomes ', 'et in a naz', 'the fabian ', 'etchy to re', ' sharman ne', 'ised empero', 'ting in pol', 'd neo latin', 'th risky ri', 'encyclopedi', 'fense the a', 'duating fro', 'treet grid ', 'ations more', 'appeal of d', 'si have mad']\n",
      "['ists advoca', 'ary governm', 'hes nationa', 'd monasteri', 'raca prince', 'chard baer ', 'rgical lang', 'for passeng', 'the nationa', 'took place ', 'ther well k', 'seven six s', 'ith a gloss', 'robably bee', 'to recogniz', 'ceived the ', 'icant than ', 'ritic of th', 'ight in sig', 's uncaused ', ' lost as in', 'cellular ic', 'e size of t', ' him a stic', 'drugs confu', ' take to co', ' the priest', 'im to name ', 'd barred at', 'standard fo', ' such as es', 'ze on the g', 'e of the or', 'd hiver one', 'y eight mar', 'the lead ch', 'es classica', 'ce the non ', 'al analysis', 'mormons bel', 't or at lea', ' disagreed ', 'ing system ', 'btypes base', 'anguages th', 'r commissio', 'ess one nin', 'nux suse li', ' the first ', 'zi concentr', ' society ne', 'elatively s', 'etworks sha', 'or hirohito', 'litical ini', 'n most of t', 'iskerdoo ri', 'ic overview', 'air compone', 'om acnm acc', ' centerline', 'e than any ', 'devotional ', 'de such dev']\n",
      "[' a']\n",
      "['an']\n"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "num_unrollings=10\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, text, batch_size, num_unrollings):\n",
    "    self._text = text\n",
    "    self._text_size = len(text)\n",
    "    self._batch_size = batch_size\n",
    "    self._num_unrollings = num_unrollings\n",
    "    segment = self._text_size // batch_size\n",
    "    self._cursor = [ offset * segment for offset in range(batch_size)]\n",
    "    self._last_batch = self._next_batch()\n",
    "  \n",
    "  def _next_batch(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float)\n",
    "    for b in range(self._batch_size):\n",
    "      batch[b, char2id(self._text[self._cursor[b]])] = 1.0\n",
    "      self._cursor[b] = (self._cursor[b] + 1) % self._text_size\n",
    "    return batch\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate the next array of batches from the data. The array consists of\n",
    "    the last batch of the previous array, followed by num_unrollings new ones.\n",
    "    \"\"\"\n",
    "    batches = [self._last_batch]\n",
    "    for step in range(self._num_unrollings):\n",
    "      batches.append(self._next_batch())\n",
    "    self._last_batch = batches[-1]\n",
    "    return batches\n",
    "\n",
    "def characters(probabilities):\n",
    "  \"\"\"Turn a 1-hot encoding or a probability distribution over the possible\n",
    "  characters back into its (most likely) character representation.\"\"\"\n",
    "  return [id2char(c) for c in np.argmax(probabilities, 1)]\n",
    "\n",
    "def batches2string(batches):\n",
    "  \"\"\"Convert a sequence of batches back into their (most likely) string\n",
    "  representation.\"\"\"\n",
    "  s = [''] * batches[0].shape[0]\n",
    "  for b in batches:\n",
    "    s = [''.join(x) for x in zip(s, characters(b))]\n",
    "  return s\n",
    "\n",
    "train_batches = BatchGenerator(train_text, batch_size, num_unrollings)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1)\n",
    "\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(train_batches.next()))\n",
    "print(batches2string(valid_batches.next()))\n",
    "print(batches2string(valid_batches.next()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "KyVd8FxT5QBc"
   },
   "outputs": [],
   "source": [
    "def logprob(predictions, labels):\n",
    "  \"\"\"Log-probability of the true labels in a predicted batch.\"\"\"\n",
    "  predictions[predictions < 1e-10] = 1e-10\n",
    "  return np.sum(np.multiply(labels, -np.log(predictions))) / labels.shape[0]\n",
    "\n",
    "def sample_distribution(distribution):\n",
    "  \"\"\"Sample one element from a distribution assumed to be an array of normalized\n",
    "  probabilities.\n",
    "  \"\"\"\n",
    "  r = random.uniform(0, 1)\n",
    "  s = 0\n",
    "  for i in range(len(distribution)):\n",
    "    s += distribution[i]\n",
    "    if s >= r:\n",
    "      return i\n",
    "  return len(distribution) - 1\n",
    "\n",
    "def sample(prediction):\n",
    "  \"\"\"Turn a (column) prediction into 1-hot encoded samples.\"\"\"\n",
    "  p = np.zeros(shape=[1, vocabulary_size], dtype=np.float)\n",
    "  p[0, sample_distribution(prediction[0])] = 1.0\n",
    "  return p\n",
    "\n",
    "def random_distribution():\n",
    "  \"\"\"Generate a random column of probabilities.\"\"\"\n",
    "  b = np.random.uniform(0.0, 1.0, size=[1, vocabulary_size])\n",
    "  return b/np.sum(b, 1)[:,None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K8f67YXaDr4C"
   },
   "source": [
    "Simple LSTM Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": false,
    "id": "Q5rxZK6RDuGe"
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  ix = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  im = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ib = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Forget gate: input, previous output, and bias.\n",
    "  fx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  fm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  fb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Memory cell: input, state and bias.                             \n",
    "  cx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  cm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  cb = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Output gate: input, previous output, and bias.\n",
    "  ox = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes], -0.1, 0.1))\n",
    "  om = tf.Variable(tf.truncated_normal([num_nodes, num_nodes], -0.1, 0.1))\n",
    "  ob = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    input_gate = tf.sigmoid(tf.matmul(i, ix) + tf.matmul(o, im) + ib)\n",
    "    forget_gate = tf.sigmoid(tf.matmul(i, fx) + tf.matmul(o, fm) + fb)\n",
    "    update = tf.matmul(i, cx) + tf.matmul(o, cm) + cb\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    output_gate = tf.sigmoid(tf.matmul(i, ox) + tf.matmul(o, om) + ob)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "    \n",
    "\n",
    "  # Optimizer.\n",
    "\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 41
      },
      {
       "item_id": 80
      },
      {
       "item_id": 126
      },
      {
       "item_id": 144
      }
     ]
    },
    "colab_type": "code",
    "collapsed": true,
    "executionInfo": {
     "elapsed": 199909,
     "status": "ok",
     "timestamp": 1445965877333,
     "user": {
      "color": "#1FA15D",
      "displayName": "Vincent Vanhoucke",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "05076109866853157986",
      "photoUrl": "//lh6.googleusercontent.com/-cCJa7dTDcgQ/AAAAAAAAAAI/AAAAAAAACgw/r2EZ_8oYer4/s50-c-k-no/photo.jpg",
      "sessionId": "6f6f07b359200c46",
      "userId": "102167687554210253930"
     },
     "user_tz": 420
    },
    "id": "RD9zQCZTEaEm",
    "outputId": "5e868466-2532-4545-ce35-b403cf5d9de6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.296025 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.01\n",
      "================================================================================\n",
      "wfebli unkxv qhol zbvcmdrtoa  prdtwdle bgoigqizpsr wdyart  tvtefcsnpxgv slngi qb\n",
      "zrrny hqaxfr zced bxpkmbuserdtifaevjdzraeyb   u  tpnic piepvg hfwikmq nicnttdaba\n",
      "tquzezokuex ebp sjqxb xrbhqter tf rn yut  zpvsewihtonoaduloi mr pcs llnonttz izr\n",
      "l e  vsoavrdtvba irrthojia ipfj mroupr   ophtro ypatawfu mrgtnhewo nloexaesyggbt\n",
      "anxbbig n y  na ebjfnwlrcsktclqntqvtogoxakpnoahe  cs sortx  enworxzio  acrvzemmx\n",
      "================================================================================\n",
      "Validation set perplexity: 20.02\n",
      "Average loss at step 100: 2.598900 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.17\n",
      "Validation set perplexity: 10.73\n",
      "Average loss at step 200: 2.233008 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.27\n",
      "Validation set perplexity: 9.23\n",
      "Average loss at step 300: 2.090917 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Validation set perplexity: 8.38\n",
      "Average loss at step 400: 2.020737 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.49\n",
      "Validation set perplexity: 7.79\n",
      "Average loss at step 500: 1.941012 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Validation set perplexity: 7.38\n",
      "Average loss at step 600: 1.911463 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 6.99\n",
      "Average loss at step 700: 1.869234 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Validation set perplexity: 6.81\n",
      "Average loss at step 800: 1.871026 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 6.55\n",
      "Average loss at step 900: 1.823637 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.37\n",
      "Validation set perplexity: 6.36\n",
      "Average loss at step 1000: 1.812119 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "================================================================================\n",
      " from risepiany up resiatinidand influst usiban andelx stolde sipped wobld daill\n",
      "wlends outhigic tymee a sovensly ogher as x one seven three chriftiless prons ve\n",
      "uring lanial camicousts and six the bersatend of a incamital workee the for the \n",
      "te sbouth siggerain resuanfriconay pressfever planding umalar mitor phies m cree\n",
      "chsity one nine eingwn two zero zero zero and inforted descest astouch fadings t\n",
      "================================================================================\n",
      "Validation set perplexity: 6.25\n",
      "Average loss at step 1100: 1.814239 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1200: 1.777980 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.05\n",
      "Validation set perplexity: 6.22\n",
      "Average loss at step 1300: 1.753717 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 5.98\n",
      "Average loss at step 1400: 1.744527 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Validation set perplexity: 6.02\n",
      "Average loss at step 1500: 1.758376 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1600: 1.753484 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 6.00\n",
      "Average loss at step 1700: 1.732570 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.95\n",
      "Validation set perplexity: 5.81\n",
      "Average loss at step 1800: 1.733959 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.83\n",
      "Average loss at step 1900: 1.698166 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.72\n",
      "Average loss at step 2000: 1.699808 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "hain of zero four seven your nine four five one six not zero zero belels the sag\n",
      "twoners in recore relonding that tut witwer clanish to rianations the endens the\n",
      "mes beling in the polities and the gatanions leven to new one nine nine nine six\n",
      "e to conows who ancherive speased and fromi farthratery to to airps hown the run\n",
      "linn sponsor decwrears craning hewraned in their of one three three seamence fis\n",
      "================================================================================\n",
      "Validation set perplexity: 5.51\n",
      "Average loss at step 2100: 1.710954 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.67\n",
      "Validation set perplexity: 5.47\n",
      "Average loss at step 2200: 1.689438 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.59\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 2300: 1.689415 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.15\n",
      "Validation set perplexity: 5.49\n",
      "Average loss at step 2400: 1.680171 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 2500: 1.665652 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2600: 1.652252 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 5.42\n",
      "Average loss at step 2700: 1.667858 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 5.44\n",
      "Average loss at step 2800: 1.659032 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 2900: 1.651889 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Validation set perplexity: 5.26\n",
      "Average loss at step 3000: 1.645636 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.30\n",
      "================================================================================\n",
      "phers the a nami clogen the two were one devemany as a hecram the for warnaylic \n",
      "t some commund diffen singled otheran gullu airopo press smaller dinter is a r y\n",
      "s also of power in facing became hirding orgent in pexince probles to his cltati\n",
      "fow american care f fiemtion a cantina chrend by name has divinginibe cited an e\n",
      "zoge en the fanizes dest this would sanch stil ob law wings layef with ide rease\n",
      "================================================================================\n",
      "Validation set perplexity: 5.18\n",
      "Average loss at step 3100: 1.648164 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.07\n",
      "Validation set perplexity: 5.48\n",
      "Average loss at step 3200: 1.644865 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Validation set perplexity: 5.39\n",
      "Average loss at step 3300: 1.625490 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.01\n",
      "Validation set perplexity: 5.33\n",
      "Average loss at step 3400: 1.603590 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.56\n",
      "Validation set perplexity: 5.22\n",
      "Average loss at step 3500: 1.655911 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.68\n",
      "Validation set perplexity: 5.34\n",
      "Average loss at step 3600: 1.647698 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 5.24\n",
      "Average loss at step 3700: 1.635577 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 5.12\n",
      "Average loss at step 3800: 1.618658 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Validation set perplexity: 5.20\n",
      "Average loss at step 3900: 1.603030 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.58\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4000: 1.621743 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "================================================================================\n",
      "qave techmugkine c oper trude s primence of a but in the side the of finly weens\n",
      "oned fehis and kined franced esulectian can arouncines of caband one six five an\n",
      "query this has that seconda alleanne chiscuby of of rise to would premeger only \n",
      "chandern embeosce an isly rolectoriel organige yecher chien wherely diffection x\n",
      "jow findy two zero like calf lipittern others ween theyer the extensed first for\n",
      "================================================================================\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 4100: 1.636795 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 5.08\n",
      "Average loss at step 4200: 1.653091 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.76\n",
      "Validation set perplexity: 5.19\n",
      "Average loss at step 4300: 1.643010 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Validation set perplexity: 5.13\n",
      "Average loss at step 4400: 1.628572 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Validation set perplexity: 5.01\n",
      "Average loss at step 4500: 1.607981 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.91\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4600: 1.599407 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.36\n",
      "Validation set perplexity: 5.15\n",
      "Average loss at step 4700: 1.631996 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 5.10\n",
      "Average loss at step 4800: 1.585264 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Validation set perplexity: 5.03\n",
      "Average loss at step 4900: 1.592499 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.65\n",
      "Validation set perplexity: 4.98\n",
      "Average loss at step 5000: 1.592375 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "================================================================================\n",
      "jood lageds most have a to and and chimas in the warch one five three chier inst\n",
      "zed lition isas at thirddess forkus britasisst postsell of zero apitne extring w\n",
      "man there to effection is harming phened tonah notestip one four zero zero zero \n",
      "waw of name photorables the altora and charries has in human notmers some reacti\n",
      "ficade light toprasecy phechories attemine sea beunds tensh one four datisculabl\n",
      "================================================================================\n",
      "Validation set perplexity: 4.96\n",
      "Average loss at step 5100: 1.577015 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.50\n",
      "Validation set perplexity: 4.87\n",
      "Average loss at step 5200: 1.585930 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.14\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5300: 1.601514 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.67\n",
      "Validation set perplexity: 4.79\n",
      "Average loss at step 5400: 1.628612 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "Validation set perplexity: 4.77\n",
      "Average loss at step 5500: 1.632330 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.87\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 5600: 1.654995 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "Validation set perplexity: 4.73\n",
      "Average loss at step 5700: 1.594792 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5800: 1.602439 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Validation set perplexity: 4.70\n",
      "Average loss at step 5900: 1.568448 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Validation set perplexity: 4.69\n",
      "Average loss at step 6000: 1.579887 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.83\n",
      "================================================================================\n",
      "el as areal dedible that broth other nised afted wire seavidual can varty and an\n",
      "rikogely rea flacentard can poviaty imonsces and the chian directibes with is to\n",
      "quedly to celloosmats fridcto lugslen vidar cyllamic of atche having doscrocgene\n",
      "by stueded two and foubble especies in the phonosan reams of the remalitarly it \n",
      "zoon freed untullant were ories jumised normality them colless geneited attom fr\n",
      "================================================================================\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6100: 1.580889 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Validation set perplexity: 4.67\n",
      "Average loss at step 6200: 1.563400 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.18\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6300: 1.612900 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.66\n",
      "Average loss at step 6400: 1.596622 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Validation set perplexity: 4.64\n",
      "Average loss at step 6500: 1.587691 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.81\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6600: 1.568339 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 6700: 1.593755 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.63\n",
      "Average loss at step 6800: 1.575938 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.56\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 6900: 1.585569 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 7000: 1.584862 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.06\n",
      "================================================================================\n",
      " he home on six one the two from novestth wisballs of unitralled accounds that s\n",
      "iled difficule molaply who chapsy at someblating or carccaphess b g not zero zer\n",
      "re the forms day for mined where brambially isration the arising chropor panabow\n",
      "mana the continity oclingt im protormates menstarta edism onered in to secort bo\n",
      "p recuarten of their one nine four ponnes a listfly was lans that carriancamene \n",
      "================================================================================\n",
      "Validation set perplexity: 4.56\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pl4vtmFfa5nn"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "You might have noticed that the definition of the LSTM cell involves 4 matrix multiplications with the input, and 4 matrix multiplications with the output. Simplify the expression by using a single matrix multiply for each, and variables that are 4 times larger.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "  \n",
    "  # Parameters:\n",
    "  # Input gate: input, previous output, and bias.\n",
    "  xx = tf.Variable(tf.truncated_normal([vocabulary_size, num_nodes*4], -0.1, 0.1))\n",
    "  mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes*4], -0.1, 0.1))\n",
    "  bb = tf.Variable(tf.zeros([1, num_nodes*4]))\n",
    "\n",
    "  # Variables saving state across unrollings.\n",
    "  saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "  # Classifier weights and biases.\n",
    "  w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "  b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "  # Definition of the cell computation.\n",
    "  def lstm_cell(i, o, state):\n",
    "    \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "    Note that in this formulation, we omit the various connections between the\n",
    "    previous state and the gates.\"\"\"\n",
    "    matmuls=tf.matmul(i,xx)+tf.matmul(o,mm)+bb\n",
    "    input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "    forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "    update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "    output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "    state = forget_gate * state + input_gate * tf.tanh(update)\n",
    "    return output_gate * tf.tanh(state), state\n",
    "\n",
    "  # Input data.\n",
    "  train_data = list()\n",
    "  for _ in range(num_unrollings + 1):\n",
    "    train_data.append(\n",
    "      tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "  train_inputs = train_data[:num_unrollings]\n",
    "  train_labels = train_data[1:]  # labels are inputs shifted by one time step.\n",
    "\n",
    "  # Unrolled LSTM loop.\n",
    "  outputs = list()\n",
    "  output = saved_output\n",
    "  state = saved_state\n",
    "  for i in train_inputs:\n",
    "    output, state = lstm_cell(i, output, state)\n",
    "    outputs.append(output)\n",
    "\n",
    "  # State saving across unrollings.\n",
    "  with tf.control_dependencies([saved_output.assign(output),\n",
    "                                saved_state.assign(state)]):\n",
    "    # Classifier.\n",
    "    logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "    loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=tf.concat(train_labels, 0), logits=logits))\n",
    "    \n",
    "\n",
    "  # Optimizer.\n",
    "\n",
    "  global_step = tf.Variable(0)\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "    10.0, global_step, 5000, 0.1, staircase=True)\n",
    "  optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "  gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "  gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "  optimizer = optimizer.apply_gradients(\n",
    "    zip(gradients, v), global_step=global_step)\n",
    "\n",
    "  # Predictions.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  # Sampling and validation eval: batch 1, no unrolling.\n",
    "  sample_input = tf.placeholder(tf.float32, shape=[1, vocabulary_size])\n",
    "  saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "  reset_sample_state = tf.group(\n",
    "    saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "    saved_sample_state.assign(tf.zeros([1, num_nodes])))\n",
    "  sample_output, sample_state = lstm_cell(\n",
    "    sample_input, saved_sample_output, saved_sample_state)\n",
    "  with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                saved_sample_state.assign(sample_state)]):\n",
    "    sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.297194 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.04\n",
      "================================================================================\n",
      "tromoveu fahetcbvcgemezwbin daroeciblnilwgobjdnllotatlngow  q sdpaeyxhlt lmnoltv\n",
      "berd uaoabhxkqaef   vuoalnsnfrzcrfccrshl qyeb plr r  xveasmhaovhxqe irmigrftvnvl\n",
      "bayr t e iakjzdnatcjnoteznuoyaufsy enlw  cri  s whlrxelxmobd trjootj  xcrnrecnv \n",
      "flamq mnuer dnfjwekt  pluunntmdpx aqjyduchtyedtaooj rtuxca nlu p irybrfreit tatv\n",
      "ooeuxicrfdfjfeitavvgcj dtrxoitobbduxvlnfoltjjna v lcb nwooo jtern bta  ufycxauny\n",
      "================================================================================\n",
      "Validation set perplexity: 20.42\n",
      "Average loss at step 1000: 2.032375 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.40\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 2000: 1.738903 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.93\n",
      "Validation set perplexity: 5.70\n",
      "Average loss at step 3000: 1.672891 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Validation set perplexity: 5.37\n",
      "Average loss at step 4000: 1.632956 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.43\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 5000: 1.611220 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.93\n",
      "Average loss at step 6000: 1.602548 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 4.72\n",
      "Average loss at step 7000: 1.595329 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.32\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 8000: 1.596748 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Validation set perplexity: 4.58\n",
      "Average loss at step 9000: 1.602258 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.11\n",
      "Validation set perplexity: 4.57\n",
      "Average loss at step 10000: 1.594970 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "kn whose u war numborticlen and the could trkimpboll nine three fore news report\n",
      "eat and one next experienced thealated polege fliger fisulationation retial pmol\n",
      "h ofference who sectemer ful and a a the first old of the hymare in one three ze\n",
      "ques dor synsherice through cacaro and preseltten attemptsical elithers fligh li\n",
      "actional polloh in micrachine of the main air desist within dorishisms to or int\n",
      "================================================================================\n",
      "Validation set perplexity: 4.65\n",
      "Average loss at step 11000: 1.559761 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.28\n",
      "Validation set perplexity: 4.59\n",
      "Average loss at step 12000: 1.567250 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.88\n",
      "Validation set perplexity: 4.55\n",
      "Average loss at step 13000: 1.579675 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.46\n",
      "Validation set perplexity: 4.52\n",
      "Average loss at step 14000: 1.592421 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.54\n",
      "Average loss at step 15000: 1.556034 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 16000: 1.559414 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 17000: 1.604891 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.62\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 18000: 1.605584 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.79\n",
      "Validation set perplexity: 4.50\n",
      "Average loss at step 19000: 1.612508 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.55\n",
      "Validation set perplexity: 4.50\n"
     ]
    }
   ],
   "source": [
    "num_steps = 20000\n",
    "summary_frequency = 1000\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  mean_loss = 0\n",
    "  for step in range(num_steps):\n",
    "    batches = train_batches.next()\n",
    "    feed_dict = dict()\n",
    "    for i in range(num_unrollings + 1):\n",
    "      feed_dict[train_data[i]] = batches[i]\n",
    "    _, l, predictions, lr = session.run(\n",
    "      [optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "    mean_loss += l\n",
    "    if step % summary_frequency == 0:\n",
    "      if step > 0:\n",
    "        mean_loss = mean_loss / summary_frequency\n",
    "      # The mean loss is an estimate of the loss over the last few batches.\n",
    "      print(\n",
    "        'Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "      mean_loss = 0\n",
    "      labels = np.concatenate(list(batches)[1:])\n",
    "      print('Minibatch perplexity: %.2f' % float(\n",
    "        np.exp(logprob(predictions, labels))))\n",
    "      if step % (summary_frequency * 10) == 0:\n",
    "        # Generate some samples.\n",
    "        print('=' * 80)\n",
    "        for _ in range(5):\n",
    "          feed = sample(random_distribution())\n",
    "          sentence = characters(feed)[0]\n",
    "          reset_sample_state.run()\n",
    "          for _ in range(79):\n",
    "            prediction = sample_prediction.eval({sample_input: feed})\n",
    "            feed = sample(prediction)\n",
    "            sentence += characters(feed)[0]\n",
    "          print(sentence)\n",
    "        print('=' * 80)\n",
    "      # Measure validation set perplexity.\n",
    "      reset_sample_state.run()\n",
    "      valid_logprob = 0\n",
    "      for _ in range(valid_size):\n",
    "        b = valid_batches.next()\n",
    "        predictions = sample_prediction.eval({sample_input: b[0]})\n",
    "        valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "      print('Validation set perplexity: %.2f' % float(np.exp(\n",
    "        valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4eErTCTybtph"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "\n",
    "We want to train a LSTM over bigrams, that is pairs of consecutive characters like 'ab' instead of single characters like 'a'. Since the number of possible bigrams is large, feeding them directly to the LSTM using 1-hot encodings will lead to a very sparse representation that is very wasteful computationally.\n",
    "\n",
    "a- Introduce an embedding lookup on the inputs, and feed the embeddings to the LSTM cell instead of the inputs themselves.\n",
    "\n",
    "b- Write a bigram-based LSTM, modeled on the character LSTM above.\n",
    "\n",
    "c- Introduce Dropout. For best practices on how to use Dropout in LSTMs, refer to this [article](http://arxiv.org/abs/1409.2329).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a) Introduce embedding lookup on the inputs, Unigram Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def idx_from_unigram_matrix(matr):\n",
    "    return matr.argmax(axis=1)\n",
    "\n",
    "\n",
    "num_nodes = 1024\n",
    "embedding_size = 30\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Input data.\n",
    "    train_inputs = []\n",
    "    train_labels = []\n",
    "    for _ in range(num_unrollings):\n",
    "        train_inputs.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeds = []\n",
    "    for ti in train_inputs:\n",
    "        embed = tf.nn.embedding_lookup(embeddings, ti)\n",
    "        train_embeds.append(embed)\n",
    "    \n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([embedding_size, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls = tf.matmul(i, xx) + tf.matmul(o, mm) + bb        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.concat(train_labels,0))\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_embed = tf.nn.embedding_lookup(embeddings, sample_input)\n",
    "    \n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.742896 learning rate: 10.000000\n",
      "Minibatch perplexity: 42.22\n",
      "================================================================================\n",
      "utb mdtzcmepcgafranthylo miltwkdbvpybrlpfcnwojjrryoplxrntveuppaqvoquorgycxdvpyrm\n",
      " szcwo jijoysvsmebrewqaarncpsoewvdnaqbodaphiboripocdcozikqydvliahjjtbipgjsvzykbf\n",
      "nzjalg vspzgtattyhprirbsrbfobyycnlsrvvoopgaimjxailpcwhlvgb mhmoyzqbamttglrmwzyho\n",
      "ltoi kownbgtlcplpsivzmrkndkyehottxhqpotvrhpbeqgpilzwqzwsbmpgbiol wpcgycjdbnzmkbo\n",
      "rxqwvhawqrimilbotgvnu qfpftdkonspvtjwmpmpbdcoujgr ncofumjsqvmuddmarffyyoniouxrey\n",
      "================================================================================\n",
      "Validation set perplexity: 39.43\n",
      "Average loss at step 100: 2.981403 learning rate: 10.000000\n",
      "Minibatch perplexity: 17.23\n",
      "Validation set perplexity: 17.17\n",
      "Average loss at step 200: 2.753631 learning rate: 10.000000\n",
      "Minibatch perplexity: 14.69\n",
      "Validation set perplexity: 15.30\n",
      "Average loss at step 300: 2.679309 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.96\n",
      "Validation set perplexity: 14.75\n",
      "Average loss at step 400: 2.640562 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.95\n",
      "Validation set perplexity: 14.20\n",
      "Average loss at step 500: 2.617235 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.49\n",
      "Validation set perplexity: 13.58\n",
      "Average loss at step 600: 2.596930 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.01\n",
      "Validation set perplexity: 13.44\n",
      "Average loss at step 700: 2.570713 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.32\n",
      "Validation set perplexity: 13.09\n",
      "Average loss at step 800: 2.543488 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.13\n",
      "Validation set perplexity: 14.08\n",
      "Average loss at step 900: 2.564664 learning rate: 10.000000\n",
      "Minibatch perplexity: 13.05\n",
      "Validation set perplexity: 12.49\n",
      "Average loss at step 1000: 2.577821 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.60\n",
      "================================================================================\n",
      "w  ofchdealrl dza di thkayep thlaine  we deger lahe ctpsr eigohdi coenjandseral \n",
      "s  cuf  weereme e c ofthivoide mil uiwrk nenge itreally  hate aed oryt r dahe gh\n",
      "y ivmanofveleremennc ainunemeftatrequrpe thus idesvee z ka arsactsi inppbyim h a\n",
      "x gmenagst oromhe e ir eyline devhisree rie hnkew osceatraivmompghge iteiaofocne\n",
      "waaupt ofw ta b thyopnitice sraterintraglatpurt iantngigm iretw wao phnesrhe nim\n",
      "================================================================================\n",
      "Validation set perplexity: 12.78\n",
      "Average loss at step 1100: 2.534905 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.20\n",
      "Validation set perplexity: 12.62\n",
      "Average loss at step 1200: 2.495885 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.25\n",
      "Validation set perplexity: 11.94\n",
      "Average loss at step 1300: 2.450303 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.37\n",
      "Validation set perplexity: 11.08\n",
      "Average loss at step 1400: 2.259671 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.68\n",
      "Validation set perplexity: 8.89\n",
      "Average loss at step 1500: 2.068929 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Validation set perplexity: 7.72\n",
      "Average loss at step 1600: 1.964400 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 7.03\n",
      "Average loss at step 1700: 1.880878 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Validation set perplexity: 6.80\n",
      "Average loss at step 1800: 1.794211 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.93\n",
      "Validation set perplexity: 6.23\n",
      "Average loss at step 1900: 1.728172 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Validation set perplexity: 5.80\n",
      "Average loss at step 2000: 1.744523 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.84\n",
      "================================================================================\n",
      "ish thus expersed sporensed dast devented cancidans twae provlestions in queter \n",
      "uars to ithistry colya briody inditions mater sevie most is probuefter beson bon\n",
      "mphotl mpashetrop dicert groughb the totald rages thooles a theinke kimed of bul\n",
      "ved cyanydix tragdine mist danker a timed unnitions ofling one nine sex one wive\n",
      "ghthis wangundira feleam beging from the vonce ethout selnaery bamerory one nine\n",
      "================================================================================\n",
      "Validation set perplexity: 5.59\n",
      "Average loss at step 2100: 1.707979 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Validation set perplexity: 5.25\n",
      "Average loss at step 2200: 1.676882 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Validation set perplexity: 5.04\n",
      "Average loss at step 2300: 1.615225 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Validation set perplexity: 4.81\n",
      "Average loss at step 2400: 1.608310 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Validation set perplexity: 4.60\n",
      "Average loss at step 2500: 1.610716 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Validation set perplexity: 4.42\n",
      "Average loss at step 2600: 1.568520 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.08\n",
      "Validation set perplexity: 4.33\n",
      "Average loss at step 2700: 1.567279 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.22\n",
      "Validation set perplexity: 4.34\n",
      "Average loss at step 2800: 1.544265 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Validation set perplexity: 4.16\n",
      "Average loss at step 2900: 1.537170 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.00\n",
      "Validation set perplexity: 4.09\n",
      "Average loss at step 3000: 1.531786 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.41\n",
      "================================================================================\n",
      "jehk  abka opposite apreally rouson a rolessordubstkn american which fether meth\n",
      "opchalle trits variiann reference the begines the took ane countic most in view \n",
      "quay for interial oreav of devimontent shing are fountce in theo two aight in fr\n",
      "by fd wan princity the de the cookned to the sandas to be through vaoration tro \n",
      "pakhche russumation were essuons traffic block protomgaturs on gonebly known the\n",
      "================================================================================\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 3100: 1.504121 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.82\n",
      "Validation set perplexity: 4.07\n",
      "Average loss at step 3200: 1.505921 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.69\n",
      "Validation set perplexity: 4.04\n",
      "Average loss at step 3300: 1.493921 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.34\n",
      "Validation set perplexity: 3.89\n",
      "Average loss at step 3400: 1.514454 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Validation set perplexity: 3.90\n",
      "Average loss at step 3500: 1.498098 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Validation set perplexity: 3.94\n",
      "Average loss at step 3600: 1.511554 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.91\n",
      "Validation set perplexity: 3.95\n",
      "Average loss at step 3700: 1.474391 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.33\n",
      "Validation set perplexity: 3.78\n",
      "Average loss at step 3800: 1.472385 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.78\n",
      "Validation set perplexity: 3.84\n",
      "Average loss at step 3900: 1.451285 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.47\n",
      "Validation set perplexity: 3.78\n",
      "Average loss at step 4000: 1.467620 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.88\n",
      "================================================================================\n",
      " spnce to kear soharish winson aron one jamoped mich his was erisims upports and\n",
      "a his hole was salemonic and gahce have that two zero zero zero zero zero yellwa\n",
      "three six zero shallow nobel writing of the ojeants for multin the reserted sigh\n",
      "er monothurs dirinitutedikas albuir color of the states out citals rushi gene of\n",
      "ers the functions which etchin toddences he panates games ected in has and ex ha\n",
      "================================================================================\n",
      "Validation set perplexity: 3.85\n",
      "Average loss at step 4100: 1.444832 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.45\n",
      "Validation set perplexity: 3.78\n",
      "Average loss at step 4200: 1.441787 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.36\n",
      "Validation set perplexity: 3.77\n",
      "Average loss at step 4300: 1.417655 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.15\n",
      "Validation set perplexity: 3.71\n",
      "Average loss at step 4400: 1.410259 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.98\n",
      "Validation set perplexity: 3.65\n",
      "Average loss at step 4500: 1.411416 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.05\n",
      "Validation set perplexity: 3.75\n",
      "Average loss at step 4600: 1.404228 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 3.74\n",
      "Average loss at step 4700: 1.406109 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.14\n",
      "Validation set perplexity: 3.75\n",
      "Average loss at step 4800: 1.414890 learning rate: 10.000000\n",
      "Minibatch perplexity: 3.54\n",
      "Validation set perplexity: 3.65\n",
      "Average loss at step 4900: 1.415498 learning rate: 10.000000\n",
      "Minibatch perplexity: 4.30\n",
      "Validation set perplexity: 3.73\n",
      "Average loss at step 5000: 1.399096 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.65\n",
      "================================================================================\n",
      "ro six zero three gaugin day with hard cansirn jepember i hawnt the heapectury i\n",
      "k it tell or one grankitets under goles also altord that was traction the blass \n",
      "d titername addition automatic bragdinj malue great clear chinase fireta organik\n",
      "ling ofservant one six zero two son unine it to parties or court in the large me\n",
      "ght to humano s right per set in manged had the contains a trafl for three five \n",
      "================================================================================\n",
      "Validation set perplexity: 3.66\n",
      "Average loss at step 5100: 1.395848 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.13\n",
      "Validation set perplexity: 3.68\n",
      "Average loss at step 5200: 1.373950 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.76\n",
      "Validation set perplexity: 3.66\n",
      "Average loss at step 5300: 1.358316 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.79\n",
      "Validation set perplexity: 3.62\n",
      "Average loss at step 5400: 1.355552 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.73\n",
      "Validation set perplexity: 3.62\n",
      "Average loss at step 5500: 1.341722 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.97\n",
      "Validation set perplexity: 3.61\n",
      "Average loss at step 5600: 1.352089 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.83\n",
      "Validation set perplexity: 3.60\n",
      "Average loss at step 5700: 1.342047 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.67\n",
      "Validation set perplexity: 3.63\n",
      "Average loss at step 5800: 1.353192 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.82\n",
      "Validation set perplexity: 3.60\n",
      "Average loss at step 5900: 1.343728 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.07\n",
      "Validation set perplexity: 3.59\n",
      "Average loss at step 6000: 1.323943 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.97\n",
      "================================================================================\n",
      " hastrine the policy and daisent force found at least the relational christiani \n",
      "an five neconscience of a commute the introduction be revided each the high remo\n",
      " one five sexing one two zero one nine six six one one eight nine three seven ha\n",
      "chllow on accomdine the profes the campaigned concentral of jerisinesbal land ou\n",
      "zeates are by memory either that go one it concentration of sygbore of it is car\n",
      "================================================================================\n",
      "Validation set perplexity: 3.61\n",
      "Average loss at step 6100: 1.339912 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.03\n",
      "Validation set perplexity: 3.62\n",
      "Average loss at step 6200: 1.305591 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.77\n",
      "Validation set perplexity: 3.62\n",
      "Average loss at step 6300: 1.316280 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.06\n",
      "Validation set perplexity: 3.56\n",
      "Average loss at step 6400: 1.309504 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.56\n",
      "Validation set perplexity: 3.56\n",
      "Average loss at step 6500: 1.320674 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.71\n",
      "Validation set perplexity: 3.56\n",
      "Average loss at step 6600: 1.357275 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.16\n",
      "Validation set perplexity: 3.52\n",
      "Average loss at step 6700: 1.329372 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.03\n",
      "Validation set perplexity: 3.62\n",
      "Average loss at step 6800: 1.361505 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.77\n",
      "Validation set perplexity: 3.60\n",
      "Average loss at step 6900: 1.330419 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.93\n",
      "Validation set perplexity: 3.61\n",
      "Average loss at step 7000: 1.319145 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.02\n",
      "================================================================================\n",
      "ant by voartiger of the result of their robinity that central monowibeit hies on\n",
      "mbo one nine six ninus translitions of one nine nine three three seven four zero\n",
      "therre local enablital resurrections wherehing the imperial senstes term its wor\n",
      "onn what is one seven eight four hearty in the bidisted amonist the kings incorp\n",
      "unin one nine two eight it is not believe minyer parenta businessman which store\n",
      "================================================================================\n",
      "Validation set perplexity: 3.55\n",
      "Average loss at step 7100: 1.335275 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.72\n",
      "Validation set perplexity: 3.58\n",
      "Average loss at step 7200: 1.322351 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.04\n",
      "Validation set perplexity: 3.60\n",
      "Average loss at step 7300: 1.312911 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.11\n",
      "Validation set perplexity: 3.65\n",
      "Average loss at step 7400: 1.329290 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.84\n",
      "Validation set perplexity: 3.63\n",
      "Average loss at step 7500: 1.329321 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.61\n",
      "Validation set perplexity: 3.60\n",
      "Average loss at step 7600: 1.295120 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.32\n",
      "Validation set perplexity: 3.57\n",
      "Average loss at step 7700: 1.285773 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.68\n",
      "Validation set perplexity: 3.51\n",
      "Average loss at step 7800: 1.309636 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.61\n",
      "Validation set perplexity: 3.56\n",
      "Average loss at step 7900: 1.311446 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.08\n",
      "Validation set perplexity: 3.58\n",
      "Average loss at step 8000: 1.338859 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.05\n",
      "================================================================================\n",
      "ch theremal te him drawnars for five domestic gaulaced five one eight eight zero\n",
      "th emperor of the web bronder to best literature tickle that were see one not fo\n",
      "ble t wothealir occasion t engineses amidercatimism for give words also linked h\n",
      "n bil forbing a planned to what and shooking kosombhi varietces from the sending\n",
      "rn increts one nine nine nine one nine eight four th enemyer government and past\n",
      "================================================================================\n",
      "Validation set perplexity: 3.52\n",
      "Average loss at step 8100: 1.326452 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.55\n",
      "Validation set perplexity: 3.60\n",
      "Average loss at step 8200: 1.296162 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 3.56\n",
      "Average loss at step 8300: 1.294587 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.48\n",
      "Validation set perplexity: 3.64\n",
      "Average loss at step 8400: 1.308381 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.67\n",
      "Validation set perplexity: 3.55\n",
      "Average loss at step 8500: 1.304801 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.76\n",
      "Validation set perplexity: 3.52\n",
      "Average loss at step 8600: 1.309179 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.61\n",
      "Validation set perplexity: 3.63\n",
      "Average loss at step 8700: 1.292458 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 3.49\n",
      "Average loss at step 8800: 1.278290 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.78\n",
      "Validation set perplexity: 3.41\n",
      "Average loss at step 8900: 1.291765 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.38\n",
      "Validation set perplexity: 3.50\n",
      "Average loss at step 9000: 1.281378 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.78\n",
      "================================================================================\n",
      "ives processes of even chronical promise announced operation and nature that wha\n",
      "mplijat one nine seven establish population one nine two spanin nationalist expe\n",
      "ffz link forsed that while a power z in like from what the half equilability usu\n",
      "us two zero zero zero k director with france kummic one nine oneen is seal weste\n",
      "y when dicterable reason with geteria belfer total atthomosahister procenue to e\n",
      "================================================================================\n",
      "Validation set perplexity: 3.43\n",
      "Average loss at step 9100: 1.289677 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.80\n",
      "Validation set perplexity: 3.44\n",
      "Average loss at step 9200: 1.300468 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.64\n",
      "Validation set perplexity: 3.40\n",
      "Average loss at step 9300: 1.308571 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.38\n",
      "Validation set perplexity: 3.45\n",
      "Average loss at step 9400: 1.297568 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.41\n",
      "Validation set perplexity: 3.44\n",
      "Average loss at step 9500: 1.308321 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.47\n",
      "Validation set perplexity: 3.46\n",
      "Average loss at step 9600: 1.286276 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.66\n",
      "Validation set perplexity: 3.40\n",
      "Average loss at step 9700: 1.298115 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.39\n",
      "Validation set perplexity: 3.47\n",
      "Average loss at step 9800: 1.311240 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.68\n",
      "Validation set perplexity: 3.49\n",
      "Average loss at step 9900: 1.304465 learning rate: 1.000000\n",
      "Minibatch perplexity: 3.56\n",
      "Validation set perplexity: 3.50\n",
      "Average loss at step 10000: 1.327428 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.54\n",
      "================================================================================\n",
      "clpas steve the relationship and disorders satellite policiasion somally and sep\n",
      "d turtie truccation to crossing to descan mutances i look islands are god to kon\n",
      "ffesbon shid is the ishually massactic emistical dirassine from attennive the ma\n",
      "quate to his polar witchart preventing a being land is a built vo in an unpulst \n",
      "ppy at went on himberg mes international testimony inside system agreement by pr\n",
      "================================================================================\n",
      "Validation set perplexity: 3.37\n",
      "Average loss at step 10100: 1.285833 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.84\n",
      "Validation set perplexity: 3.46\n",
      "Average loss at step 10200: 1.274248 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 3.45\n",
      "Average loss at step 10300: 1.288897 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.70\n",
      "Validation set perplexity: 3.51\n",
      "Average loss at step 10400: 1.281269 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.40\n",
      "Validation set perplexity: 3.46\n",
      "Average loss at step 10500: 1.319708 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.64\n",
      "Validation set perplexity: 3.48\n",
      "Average loss at step 10600: 1.299741 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.76\n",
      "Validation set perplexity: 3.49\n",
      "Average loss at step 10700: 1.288482 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.93\n",
      "Validation set perplexity: 3.43\n",
      "Average loss at step 10800: 1.298336 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 3.39\n",
      "Average loss at step 10900: 1.298961 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.61\n",
      "Validation set perplexity: 3.38\n",
      "Average loss at step 11000: 1.283047 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.36\n",
      "================================================================================\n",
      "us neghiltenskeron hospield herisons breen inserved synford on the significantly\n",
      "k y progress thought the why fell whether as geerli have be a late the wrote to \n",
      "bery atena studenti game missile one of the progress fictional monetaulle the na\n",
      "d center of kreidy michamp as agendson left it game zombited as a mormis diction\n",
      "barkice deaths other vandex of the crall suz hc ande afer one five ale for the d\n",
      "================================================================================\n",
      "Validation set perplexity: 3.38\n",
      "Average loss at step 11100: 1.272769 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.30\n",
      "Validation set perplexity: 3.31\n",
      "Average loss at step 11200: 1.269449 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.78\n",
      "Validation set perplexity: 3.39\n",
      "Average loss at step 11300: 1.267873 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.89\n",
      "Validation set perplexity: 3.38\n",
      "Average loss at step 11400: 1.277423 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.57\n",
      "Validation set perplexity: 3.40\n",
      "Average loss at step 11500: 1.299245 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.90\n",
      "Validation set perplexity: 3.42\n",
      "Average loss at step 11600: 1.272665 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.10\n",
      "Validation set perplexity: 3.41\n",
      "Average loss at step 11700: 1.278084 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.49\n",
      "Validation set perplexity: 3.42\n",
      "Average loss at step 11800: 1.263144 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.60\n",
      "Validation set perplexity: 3.47\n",
      "Average loss at step 11900: 1.281570 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.88\n",
      "Validation set perplexity: 3.34\n",
      "Average loss at step 12000: 1.282766 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.48\n",
      "================================================================================\n",
      "rocs actor our running fragments in itsuchs map divergence cional names the earl\n",
      "ttel c one the dept the pronunciate accounts of modern geneva and these succlair\n",
      "two one six four a severely several the ve of genital wound emperor of emmu shen\n",
      "ly simpler size is lande is ever swiss state andresson wad szepheld sports of th\n",
      "long microsachevil partisan anathime sectogs that evolutionance and the highest \n",
      "================================================================================\n",
      "Validation set perplexity: 3.40\n",
      "Average loss at step 12100: 1.269331 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.47\n",
      "Validation set perplexity: 3.48\n",
      "Average loss at step 12200: 1.263274 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.51\n",
      "Validation set perplexity: 3.38\n",
      "Average loss at step 12300: 1.278834 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.60\n",
      "Validation set perplexity: 3.45\n",
      "Average loss at step 12400: 1.267289 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.43\n",
      "Validation set perplexity: 3.37\n",
      "Average loss at step 12500: 1.265202 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.76\n",
      "Validation set perplexity: 3.43\n",
      "Average loss at step 12600: 1.249414 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.23\n",
      "Validation set perplexity: 3.40\n",
      "Average loss at step 12700: 1.253946 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.52\n",
      "Validation set perplexity: 3.33\n",
      "Average loss at step 12800: 1.285609 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.47\n",
      "Validation set perplexity: 3.39\n",
      "Average loss at step 12900: 1.258877 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.26\n",
      "Validation set perplexity: 3.41\n",
      "Average loss at step 13000: 1.261993 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.48\n",
      "================================================================================\n",
      "ly carry the dominant of the long neoll was march one five million corporating n\n",
      "viy catholic circulation from this state from his book one nine three ninehent e\n",
      "an colombia the vio all richards air raining icons without at the east german je\n",
      "ar language of which four two because the hit sundanced germen see also the eart\n",
      "quax different journal notes rumor mon rise beniau area two zero two zero metres\n",
      "================================================================================\n",
      "Validation set perplexity: 3.38\n",
      "Average loss at step 13100: 1.249248 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.56\n",
      "Validation set perplexity: 3.42\n",
      "Average loss at step 13200: 1.260268 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.58\n",
      "Validation set perplexity: 3.50\n",
      "Average loss at step 13300: 1.261288 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.75\n",
      "Validation set perplexity: 3.51\n",
      "Average loss at step 13400: 1.256869 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.44\n",
      "Validation set perplexity: 3.52\n",
      "Average loss at step 13500: 1.231678 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.53\n",
      "Validation set perplexity: 3.48\n",
      "Average loss at step 13600: 1.259995 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 3.44\n",
      "Average loss at step 13700: 1.249074 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.88\n",
      "Validation set perplexity: 3.44\n",
      "Average loss at step 13800: 1.248089 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.30\n",
      "Validation set perplexity: 3.44\n",
      "Average loss at step 13900: 1.240963 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.23\n",
      "Validation set perplexity: 3.30\n",
      "Average loss at step 14000: 1.255110 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.46\n",
      "================================================================================\n",
      "d one seven zero november two meshing bombarding however proof times who would b\n",
      "d five swiss around two zero zero one recease on a railway when it s religious w\n",
      "s ates the series in celpuit cissing yazkada athonist albani name fluiding its l\n",
      "quaard musicacchip browser it is preyider on monkay used former region wing mary\n",
      "jozs impoped mid at a defendant forum a deal with fields of one clibordman s cre\n",
      "================================================================================\n",
      "Validation set perplexity: 3.32\n",
      "Average loss at step 14100: 1.267386 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.19\n",
      "Validation set perplexity: 3.42\n",
      "Average loss at step 14200: 1.241438 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.18\n",
      "Validation set perplexity: 3.37\n",
      "Average loss at step 14300: 1.233939 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.38\n",
      "Validation set perplexity: 3.37\n",
      "Average loss at step 14400: 1.225367 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 3.35\n",
      "Average loss at step 14500: 1.256148 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.81\n",
      "Validation set perplexity: 3.39\n",
      "Average loss at step 14600: 1.244212 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.87\n",
      "Validation set perplexity: 3.34\n",
      "Average loss at step 14700: 1.238047 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.41\n",
      "Validation set perplexity: 3.41\n",
      "Average loss at step 14800: 1.262434 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.45\n",
      "Validation set perplexity: 3.47\n",
      "Average loss at step 14900: 1.276795 learning rate: 0.100000\n",
      "Minibatch perplexity: 3.71\n",
      "Validation set perplexity: 3.42\n",
      "Average loss at step 15000: 1.212680 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.28\n",
      "================================================================================\n",
      "jtzn with died the popular of the written being department aquils the one nine n\n",
      "wis x six jean years against the slaveway at the one nine nine two the same the \n",
      "d fanction scheduled guapper with the european pacedure chr smoke allowing the s\n",
      "ctquavens commodore outnegveeners we lost on one seven one eight nine three one \n",
      "y and the risk announces a library canadian paic two zero zero two isbn one nine\n",
      "================================================================================\n",
      "Validation set perplexity: 3.39\n",
      "Average loss at step 15100: 1.219891 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.44\n",
      "Validation set perplexity: 3.42\n",
      "Average loss at step 15200: 1.256135 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.23\n",
      "Validation set perplexity: 3.36\n",
      "Average loss at step 15300: 1.235483 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.50\n",
      "Validation set perplexity: 3.29\n",
      "Average loss at step 15400: 1.226593 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.04\n",
      "Validation set perplexity: 3.39\n",
      "Average loss at step 15500: 1.220580 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.64\n",
      "Validation set perplexity: 3.34\n",
      "Average loss at step 15600: 1.243049 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.39\n",
      "Validation set perplexity: 3.35\n",
      "Average loss at step 15700: 1.277007 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.77\n",
      "Validation set perplexity: 3.34\n",
      "Average loss at step 15800: 1.242670 learning rate: 0.010000\n",
      "Minibatch perplexity: 2.94\n",
      "Validation set perplexity: 3.35\n",
      "Average loss at step 15900: 1.255011 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.37\n",
      "Validation set perplexity: 3.35\n",
      "Average loss at step 16000: 1.262982 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.88\n",
      "================================================================================\n",
      "ble no civil s ones and television societies and main way solved as for elemant \n",
      "qudttere recording evarked by these liberty and gisbuke of the tucanobis it can \n",
      "cuor information and the such alternative has roman republican inherital resorts\n",
      "jaze per carbon dioxide golds some over an electric donatists the population one\n",
      "c desis mysot british rkgeling terry vi the varia deathbrato had mikey cartoons \n",
      "================================================================================\n",
      "Validation set perplexity: 3.42\n",
      "Average loss at step 16100: 1.259080 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.09\n",
      "Validation set perplexity: 3.42\n",
      "Average loss at step 16200: 1.236052 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.49\n",
      "Validation set perplexity: 3.45\n",
      "Average loss at step 16300: 1.241968 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.37\n",
      "Validation set perplexity: 3.37\n",
      "Average loss at step 16400: 1.246308 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.71\n",
      "Validation set perplexity: 3.27\n",
      "Average loss at step 16500: 1.279596 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.72\n",
      "Validation set perplexity: 3.30\n",
      "Average loss at step 16600: 1.254737 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.41\n",
      "Validation set perplexity: 3.31\n",
      "Average loss at step 16700: 1.269164 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.57\n",
      "Validation set perplexity: 3.27\n",
      "Average loss at step 16800: 1.235665 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 3.32\n",
      "Average loss at step 16900: 1.242253 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.73\n",
      "Validation set perplexity: 3.29\n",
      "Average loss at step 17000: 1.240545 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.36\n",
      "================================================================================\n",
      "ving flight to out and needed based antipopes it is an agricultural associations\n",
      "and inbellignts lockhisk copyright non persists in rusper like williams and klar\n",
      "mentios theory soil land smads ton smowed three three zero three it is beginning\n",
      "mentary air born a major civil war circumstances from the rasing may have a regi\n",
      "n leneta sparks and translation with two zero zero three cleature fully done on \n",
      "================================================================================\n",
      "Validation set perplexity: 3.24\n",
      "Average loss at step 17100: 1.245271 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.18\n",
      "Validation set perplexity: 3.24\n",
      "Average loss at step 17200: 1.243865 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.43\n",
      "Validation set perplexity: 3.26\n",
      "Average loss at step 17300: 1.231614 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.54\n",
      "Validation set perplexity: 3.31\n",
      "Average loss at step 17400: 1.227326 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.35\n",
      "Validation set perplexity: 3.26\n",
      "Average loss at step 17500: 1.230236 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.16\n",
      "Validation set perplexity: 3.38\n",
      "Average loss at step 17600: 1.221831 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.73\n",
      "Validation set perplexity: 3.31\n",
      "Average loss at step 17700: 1.232605 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.53\n",
      "Validation set perplexity: 3.32\n",
      "Average loss at step 17800: 1.223908 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 3.25\n",
      "Average loss at step 17900: 1.234506 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.31\n",
      "Validation set perplexity: 3.32\n",
      "Average loss at step 18000: 1.232069 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.38\n",
      "================================================================================\n",
      "waimght one nine nine as cls estreet a clear hour went as a religious by saying \n",
      "gh urh peruitrela image of the minor jack windows xp sexual automatist smytholog\n",
      "it pioneer darkers yen award on the team however elion from the command global w\n",
      "ned cio and game remaining st clean duringia canadma was kai and for monastery a\n",
      "wa cait destram case one eight knoceltes may know from the occasions in glin joh\n",
      "================================================================================\n",
      "Validation set perplexity: 3.25\n",
      "Average loss at step 18100: 1.231563 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.36\n",
      "Validation set perplexity: 3.28\n",
      "Average loss at step 18200: 1.207524 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.43\n",
      "Validation set perplexity: 3.27\n",
      "Average loss at step 18300: 1.215655 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.56\n",
      "Validation set perplexity: 3.33\n",
      "Average loss at step 18400: 1.227903 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.18\n",
      "Validation set perplexity: 3.30\n",
      "Average loss at step 18500: 1.242124 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.58\n",
      "Validation set perplexity: 3.32\n",
      "Average loss at step 18600: 1.245457 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.51\n",
      "Validation set perplexity: 3.26\n",
      "Average loss at step 18700: 1.215001 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.43\n",
      "Validation set perplexity: 3.32\n",
      "Average loss at step 18800: 1.206839 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.37\n",
      "Validation set perplexity: 3.33\n",
      "Average loss at step 18900: 1.209459 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.18\n",
      "Validation set perplexity: 3.29\n",
      "Average loss at step 19000: 1.241455 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.56\n",
      "================================================================================\n",
      "ne worldrides velis and blues can require three jupiter one nine seven five thre\n",
      "s rnage bc seat marino g ceu brungers one luxembouldi of the battle was filmed t\n",
      "jrzmpleson two zero zero five blue jeun eight three five one zero two zero zero \n",
      " due to defense to gay kar three three nine t amnesus river and standard tata sa\n",
      "y foux is straig as the touch by the wild gia types of site creating that enarts\n",
      "================================================================================\n",
      "Validation set perplexity: 3.33\n",
      "Average loss at step 19100: 1.257817 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.52\n",
      "Validation set perplexity: 3.31\n",
      "Average loss at step 19200: 1.252234 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.79\n",
      "Validation set perplexity: 3.27\n",
      "Average loss at step 19300: 1.229110 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.37\n",
      "Validation set perplexity: 3.30\n",
      "Average loss at step 19400: 1.232515 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.73\n",
      "Validation set perplexity: 3.32\n",
      "Average loss at step 19500: 1.227393 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.23\n",
      "Validation set perplexity: 3.30\n",
      "Average loss at step 19600: 1.239042 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.36\n",
      "Validation set perplexity: 3.29\n",
      "Average loss at step 19700: 1.214150 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.82\n",
      "Validation set perplexity: 3.32\n",
      "Average loss at step 19800: 1.214086 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.31\n",
      "Validation set perplexity: 3.30\n",
      "Average loss at step 19900: 1.221891 learning rate: 0.010000\n",
      "Minibatch perplexity: 3.40\n",
      "Validation set perplexity: 3.33\n",
      "Average loss at step 20000: 1.203210 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.15\n",
      "================================================================================\n",
      "gh one compound to ten pedhard presidents of alexander and corraphorest and nine\n",
      "ng part often states for example the minimum wage together with portraying fume \n",
      "us one two nine two one pat s giving sportsky village and transact and bridges f\n",
      "dony of lenders kon of kmaglie pala s british monarchy gibraltan of inherpennsul\n",
      "w es loudli day american lerisoders strong but the scales of three four however \n",
      "================================================================================\n",
      "Validation set perplexity: 3.35\n",
      "Average loss at step 20100: 1.220030 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.29\n",
      "Validation set perplexity: 3.37\n",
      "Average loss at step 20200: 1.235693 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.37\n",
      "Validation set perplexity: 3.29\n",
      "Average loss at step 20300: 1.241488 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.64\n",
      "Validation set perplexity: 3.34\n",
      "Average loss at step 20400: 1.274810 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.58\n",
      "Validation set perplexity: 3.25\n",
      "Average loss at step 20500: 1.279516 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.59\n",
      "Validation set perplexity: 3.21\n",
      "Average loss at step 20600: 1.244168 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.23\n",
      "Validation set perplexity: 3.25\n",
      "Average loss at step 20700: 1.238716 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 3.27\n",
      "Average loss at step 20800: 1.227856 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.63\n",
      "Validation set perplexity: 3.32\n",
      "Average loss at step 20900: 1.221201 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.37\n",
      "Validation set perplexity: 3.35\n",
      "Average loss at step 21000: 1.229078 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.31\n",
      "================================================================================\n",
      "vies known for the gallis st john the gelm alead claimed known as bma were bit s\n",
      "gn man and its times one nine seven six four one and the duncan firelians cowes \n",
      "d popular lands to pair hyle rejudance i in each macbing age one eight three six\n",
      "f  one nine zero one three new york d one nine nine hogarde satan actors three s\n",
      "ro famous ruzburg games were monopoly effectively detectively no operated in the\n",
      "================================================================================\n",
      "Validation set perplexity: 3.20\n",
      "Average loss at step 21100: 1.207427 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.29\n",
      "Validation set perplexity: 3.20\n",
      "Average loss at step 21200: 1.240026 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.72\n",
      "Validation set perplexity: 3.18\n",
      "Average loss at step 21300: 1.234308 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.31\n",
      "Validation set perplexity: 3.19\n",
      "Average loss at step 21400: 1.222405 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.49\n",
      "Validation set perplexity: 3.19\n",
      "Average loss at step 21500: 1.210815 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.40\n",
      "Validation set perplexity: 3.20\n",
      "Average loss at step 21600: 1.215738 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.07\n",
      "Validation set perplexity: 3.20\n",
      "Average loss at step 21700: 1.219406 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.43\n",
      "Validation set perplexity: 3.17\n",
      "Average loss at step 21800: 1.218081 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.50\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 21900: 1.227809 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.31\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 22000: 1.211798 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.49\n",
      "================================================================================\n",
      "ong fi giantage note the league autocratt constituence bead and the barded were \n",
      "y are forced and abeitions were one one zero zero years explained by review runn\n",
      "fright triol in the one five two problems notes and about two four two zero zero\n",
      "ing five six events only eight zero bhole one eight two one eight nine nine was \n",
      "d citizens diknogius is a compuls heur doube one for a top living localization o\n",
      "================================================================================\n",
      "Validation set perplexity: 3.13\n",
      "Average loss at step 22100: 1.213372 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.55\n",
      "Validation set perplexity: 3.10\n",
      "Average loss at step 22200: 1.234583 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.43\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 22300: 1.235732 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.05\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 22400: 1.217197 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 22500: 1.211555 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.28\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 22600: 1.201581 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.31\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 22700: 1.208834 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.71\n",
      "Validation set perplexity: 3.06\n",
      "Average loss at step 22800: 1.225287 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.52\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 22900: 1.221440 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.87\n",
      "Validation set perplexity: 3.03\n",
      "Average loss at step 23000: 1.234117 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.20\n",
      "================================================================================\n",
      "vistablize press in rome finally months humor of the first gense nose that merte\n",
      "biling british greek pograms te first and sheaks would not reach the capital sto\n",
      "ts fielda was enter that with support sexual facilities after the enormous and a\n",
      "n fortunity on l winter became fital to other churches ahead chewina political c\n",
      "g kancy are also a women drile videothazer reflact of parallel indianosephile us\n",
      "================================================================================\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 23100: 1.218946 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.50\n",
      "Validation set perplexity: 3.08\n",
      "Average loss at step 23200: 1.243100 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.17\n",
      "Validation set perplexity: 3.13\n",
      "Average loss at step 23300: 1.236511 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.15\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 23400: 1.222233 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.29\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 23500: 1.227606 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.45\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 23600: 1.223968 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.44\n",
      "Validation set perplexity: 3.17\n",
      "Average loss at step 23700: 1.207701 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.54\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 23800: 1.208647 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 3.13\n",
      "Average loss at step 23900: 1.212039 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.63\n",
      "Validation set perplexity: 3.18\n",
      "Average loss at step 24000: 1.219716 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.40\n",
      "================================================================================\n",
      "ing menter harm thomgo in two zero zero two one three at due to which movement t\n",
      "ops intelligence related works the baroque people named beagned by the rome and \n",
      "ha formed an omanus american people suthead of in jiny which golfer crit lawyer \n",
      " in one five zero s six de each of the one seven eight two zero zero two fractic\n",
      "ing disciplines shows the young engage to scale on foreign in the two zero th ce\n",
      "================================================================================\n",
      "Validation set perplexity: 3.22\n",
      "Average loss at step 24100: 1.197128 learning rate: 0.001000\n",
      "Minibatch perplexity: 2.99\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 24200: 1.220888 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.68\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 24300: 1.207361 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.77\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 24400: 1.214960 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.50\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 24500: 1.214790 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 24600: 1.215241 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.11\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 24700: 1.211252 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.17\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 24800: 1.208964 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.41\n",
      "Validation set perplexity: 3.17\n",
      "Average loss at step 24900: 1.200390 learning rate: 0.001000\n",
      "Minibatch perplexity: 3.22\n",
      "Validation set perplexity: 3.11\n",
      "Average loss at step 25000: 1.209715 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.43\n",
      "================================================================================\n",
      "d coffee the flipple country wishing to will key most like a maintenad numbers o\n",
      "y one nine nine zero one nine one two aid for national himalia proplicity films \n",
      "ly exclusively to promulate to a page pit neighborridue often by josephi muse of\n",
      "zzat van i jah bragvel vi whanner her life and dance s political society knowled\n",
      "d familiar spread brestread trustology calling the barisee billion aa and ebora \n",
      "================================================================================\n",
      "Validation set perplexity: 3.10\n",
      "Average loss at step 25100: 1.214011 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.86\n",
      "Validation set perplexity: 3.10\n",
      "Average loss at step 25200: 1.212427 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 25300: 1.184064 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.22\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 25400: 1.199819 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.37\n",
      "Validation set perplexity: 3.01\n",
      "Average loss at step 25500: 1.195621 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.21\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 25600: 1.196887 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.35\n",
      "Validation set perplexity: 3.10\n",
      "Average loss at step 25700: 1.196061 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.45\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 25800: 1.204963 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.48\n",
      "Validation set perplexity: 3.10\n",
      "Average loss at step 25900: 1.208795 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.18\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 26000: 1.226062 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.06\n",
      "================================================================================\n",
      "vi four zero four s head and eventually little also all of the formal developmen\n",
      "es eight years one six one six comparable ending one nine six four spulffle gela\n",
      "queng level on the queen ude humanist district returned to give there st malism \n",
      "x four two in the state as portuguesed a creation for his irradophar moroovo bil\n",
      "y if i these critics the executing nation may who have fixed marked to make the \n",
      "================================================================================\n",
      "Validation set perplexity: 3.10\n",
      "Average loss at step 26100: 1.206056 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.37\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 26200: 1.222815 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.07\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 26300: 1.199487 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.09\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 26400: 1.193469 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 3.06\n",
      "Average loss at step 26500: 1.198816 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.58\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 26600: 1.195560 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.12\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 26700: 1.211657 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.20\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 26800: 1.202337 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.85\n",
      "Validation set perplexity: 3.10\n",
      "Average loss at step 26900: 1.212258 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.24\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 27000: 1.186101 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.18\n",
      "================================================================================\n",
      "roas often importance of the second alpine further additional resolution to thei\n",
      "blamz some begin a six five six can become gaul and honda d order a later six ze\n",
      "kq one nine six one hours britain franklin is model s counternet claims by watch\n",
      "s don life products such as course one sax seized two zero zero one prices for f\n",
      "orand isbn zero six one five six one five six one five million one nine two two \n",
      "================================================================================\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 27100: 1.219850 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.56\n",
      "Validation set perplexity: 3.20\n",
      "Average loss at step 27200: 1.223980 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.23\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 27300: 1.222871 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.31\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 27400: 1.214133 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.24\n",
      "Validation set perplexity: 3.22\n",
      "Average loss at step 27500: 1.218958 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.54\n",
      "Validation set perplexity: 3.28\n",
      "Average loss at step 27600: 1.196629 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.30\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 27700: 1.204778 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.11\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 27800: 1.217483 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.48\n",
      "Validation set perplexity: 3.20\n",
      "Average loss at step 27900: 1.235416 learning rate: 0.000100\n",
      "Minibatch perplexity: 2.89\n",
      "Validation set perplexity: 3.22\n",
      "Average loss at step 28000: 1.224092 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.25\n",
      "================================================================================\n",
      "onor relatives in one nine two eight he worst articipropic latitude ring officia\n",
      "vies the actress openance in sax furriel kantson her mohilane balkans new bergen\n",
      "fifrqund eight two zero zero five to adequate being renewal in julius the world \n",
      "nis ice mwa however and peak isbn one eight he liver being one eight seven eight\n",
      "x one three three bc cris convokees not to member of the diosez id in his son wr\n",
      "================================================================================\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 28100: 1.217632 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.31\n",
      "Validation set perplexity: 3.18\n",
      "Average loss at step 28200: 1.208633 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.52\n",
      "Validation set perplexity: 3.08\n",
      "Average loss at step 28300: 1.204747 learning rate: 0.000100\n",
      "Minibatch perplexity: 2.96\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 28400: 1.209516 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.81\n",
      "Validation set perplexity: 3.11\n",
      "Average loss at step 28500: 1.210802 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.10\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 28600: 1.195551 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.48\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 28700: 1.206386 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.50\n",
      "Validation set perplexity: 3.22\n",
      "Average loss at step 28800: 1.205384 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.57\n",
      "Validation set perplexity: 3.22\n",
      "Average loss at step 28900: 1.206472 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.17\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 29000: 1.202935 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.77\n",
      "================================================================================\n",
      "x four four one four five balboa construmently phesis compute t aerobation lysic\n",
      "he de quart dauge was true one seven hamascus together of czes were in the two z\n",
      "unth followed as well as prior rand andrest classical time these of being archip\n",
      "waleus rebc winoalamis sa heterogy in two zero zero four an active and the focus\n",
      "d tamil and lose post disco that an integral cost konals roast has administrativ\n",
      "================================================================================\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 29100: 1.202069 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.36\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 29200: 1.203127 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.30\n",
      "Validation set perplexity: 3.10\n",
      "Average loss at step 29300: 1.199449 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.16\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 29400: 1.203157 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.75\n",
      "Validation set perplexity: 3.11\n",
      "Average loss at step 29500: 1.219791 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.65\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 29600: 1.218574 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.20\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 29700: 1.194814 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.39\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 29800: 1.226838 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.47\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 29900: 1.230704 learning rate: 0.000100\n",
      "Minibatch perplexity: 3.51\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 30000: 1.198040 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.59\n",
      "================================================================================\n",
      "denpays di marx russles i see i adal related during crava toon secur by the moth\n",
      "e zuch one nine nine six book stereoucs a garag sauka khan aired in the northeas\n",
      "er dishelming in praying ha center google three k a straight africa prifitation \n",
      "jujprhight has several the amazon jour the righting in protest poverty of algede\n",
      "hazn usk toneo was one of the complain four of the second system home instead ex\n",
      "================================================================================\n",
      "Validation set perplexity: 3.22\n",
      "Average loss at step 30100: 1.191000 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.10\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 30200: 1.185237 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.11\n",
      "Validation set perplexity: 3.19\n",
      "Average loss at step 30300: 1.226050 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.89\n",
      "Validation set perplexity: 3.22\n",
      "Average loss at step 30400: 1.216683 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.53\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 30500: 1.191301 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.51\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 30600: 1.192903 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.22\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 30700: 1.196166 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.26\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 30800: 1.205145 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.35\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 30900: 1.181807 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.19\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 31000: 1.174495 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.28\n",
      "================================================================================\n",
      "fry conspicuoued blood of encoding a big budget it consider uncertaining on a re\n",
      "f long to seven first classics are larger many criter mail those constructions a\n",
      "qujs ne actually tendates and mystical army word of two zero zero zero one five \n",
      "er essayists sounds the pregnient was the television the inuit in an idis by ca \n",
      "an soldient urourian dialect figure in two zero zero three j js see also metzote\n",
      "================================================================================\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 31100: 1.212327 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.69\n",
      "Validation set perplexity: 3.17\n",
      "Average loss at step 31200: 1.212892 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.38\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 31300: 1.198834 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.54\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 31400: 1.192185 learning rate: 0.000010\n",
      "Minibatch perplexity: 4.09\n",
      "Validation set perplexity: 3.18\n",
      "Average loss at step 31500: 1.178157 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.19\n",
      "Validation set perplexity: 3.10\n",
      "Average loss at step 31600: 1.169283 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.87\n",
      "Validation set perplexity: 3.20\n",
      "Average loss at step 31700: 1.190332 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.18\n",
      "Validation set perplexity: 3.23\n",
      "Average loss at step 31800: 1.183802 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.27\n",
      "Validation set perplexity: 3.18\n",
      "Average loss at step 31900: 1.205748 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.37\n",
      "Validation set perplexity: 3.13\n",
      "Average loss at step 32000: 1.179387 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.21\n",
      "================================================================================\n",
      "the lake of saint o the igor groups to stop settlement and a following time for \n",
      "billin two zero zero six was acting or produced the move its infattresdler s ini\n",
      "qujing the ansapopital has arranged to return after various personal generated o\n",
      "forts her fernang to the son external links asteroid word wing eight five seven \n",
      "kqizgipro i key digital velatians with company regulated as disastenau concepts \n",
      "================================================================================\n",
      "Validation set perplexity: 3.10\n",
      "Average loss at step 32100: 1.198829 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 32200: 1.172768 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.39\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 32300: 1.179792 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.00\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 32400: 1.196469 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.02\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 32500: 1.190219 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.34\n",
      "Validation set perplexity: 3.10\n",
      "Average loss at step 32600: 1.183797 learning rate: 0.000010\n",
      "Minibatch perplexity: 2.94\n",
      "Validation set perplexity: 3.17\n",
      "Average loss at step 32700: 1.179578 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.66\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 32800: 1.206414 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 3.19\n",
      "Average loss at step 32900: 1.183641 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.02\n",
      "Validation set perplexity: 3.20\n",
      "Average loss at step 33000: 1.197858 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.38\n",
      "================================================================================\n",
      "th however a should not stayed food took eleven discipline is a state acts did l\n",
      "or average in union referring to banks one nine three two five six six two darit\n",
      "waheseness occupation spoke at objects re also associated with a led him nostah \n",
      "f joxity of brothers one nine four nine children of numerous water bobig austral\n",
      "n peison finest and warship and painters points and trade disciplines and uses o\n",
      "================================================================================\n",
      "Validation set perplexity: 3.20\n",
      "Average loss at step 33100: 1.203186 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.62\n",
      "Validation set perplexity: 3.17\n",
      "Average loss at step 33200: 1.188433 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.16\n",
      "Validation set perplexity: 3.18\n",
      "Average loss at step 33300: 1.200832 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.18\n",
      "Validation set perplexity: 3.18\n",
      "Average loss at step 33400: 1.185674 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.35\n",
      "Validation set perplexity: 3.20\n",
      "Average loss at step 33500: 1.197983 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.23\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 33600: 1.200554 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.77\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 33700: 1.192110 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.15\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 33800: 1.185352 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.18\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 33900: 1.186657 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.12\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 34000: 1.186799 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.42\n",
      "================================================================================\n",
      "y one seven order but also well defeated away on nation with us live machine bel\n",
      "gh ountially region as a property and defined to kill hephodical voice got awrit\n",
      "and walt coincided with the three four dollars as the king and the female and an\n",
      "less stegs were militianly one of the country but carefully college in the old q\n",
      " he vrew someone and poet symbol is settled in one st he ir searl v v rkor and p\n",
      "================================================================================\n",
      "Validation set perplexity: 3.10\n",
      "Average loss at step 34100: 1.214499 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 34200: 1.200196 learning rate: 0.000010\n",
      "Minibatch perplexity: 2.94\n",
      "Validation set perplexity: 3.11\n",
      "Average loss at step 34300: 1.200247 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.29\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 34400: 1.220374 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.55\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 34500: 1.234905 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.46\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 34600: 1.201957 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.32\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 34700: 1.204941 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.05\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 34800: 1.195688 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.38\n",
      "Validation set perplexity: 3.20\n",
      "Average loss at step 34900: 1.164759 learning rate: 0.000010\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 35000: 1.188900 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.28\n",
      "================================================================================\n",
      "qujm andros do nor maintaining stockade would be also thought inspired itself vi\n",
      "leb and st principal publishers australian open champion one nine five nine logc\n",
      "d one seven two zero strings were eleven in the one eight one seven found in mor\n",
      "wi participation on smalling to log while the mountain it led to the threw by th\n",
      "d not necenthe rouging channel graduates douglas that there is no santium based \n",
      "================================================================================\n",
      "Validation set perplexity: 3.11\n",
      "Average loss at step 35100: 1.198254 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.65\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 35200: 1.187186 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.49\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 35300: 1.200882 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.12\n",
      "Validation set perplexity: 3.15\n",
      "Average loss at step 35400: 1.192067 learning rate: 0.000001\n",
      "Minibatch perplexity: 2.91\n",
      "Validation set perplexity: 3.13\n",
      "Average loss at step 35500: 1.184170 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.30\n",
      "Validation set perplexity: 3.11\n",
      "Average loss at step 35600: 1.174218 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.48\n",
      "Validation set perplexity: 3.06\n",
      "Average loss at step 35700: 1.183654 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.06\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 35800: 1.201869 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.59\n",
      "Validation set perplexity: 3.06\n",
      "Average loss at step 35900: 1.215601 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.44\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 36000: 1.197190 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.57\n",
      "================================================================================\n",
      "and leaders changes history his samples internaction and homosexuality occup int\n",
      "hijet in senatory addiction rata systems with the delasiming to salt success sho\n",
      "worjbeg aed was the study of aggression appamated on october th century natural \n",
      "vic aesthatinoplox report to the blade throughout the korajs inflation and compa\n",
      "he es in the ceftaq sovereignty of their treaty issue when anuson san and the di\n",
      "================================================================================\n",
      "Validation set perplexity: 3.07\n",
      "Average loss at step 36100: 1.193423 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.17\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 36200: 1.185606 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.43\n",
      "Validation set perplexity: 3.03\n",
      "Average loss at step 36300: 1.204664 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.58\n",
      "Validation set perplexity: 3.06\n",
      "Average loss at step 36400: 1.187908 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.04\n",
      "Validation set perplexity: 3.07\n",
      "Average loss at step 36500: 1.208776 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.45\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 36600: 1.215903 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.07\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 36700: 1.198444 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 3.17\n",
      "Average loss at step 36800: 1.209821 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.87\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 36900: 1.191299 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.28\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 37000: 1.205384 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.35\n",
      "================================================================================\n",
      "x from they successful estimated to earth louis the first government d one nine \n",
      "utorious organic capone saints and tales kuwait completed cults to group which r\n",
      "prrain and island trolley engaging the fallmoot is novena although he apsetical \n",
      "row the bied the actual church converge in the establishment for uimoits e in et\n",
      "x playing the forces with spheros in matter or south end of the swamped cellar j\n",
      "================================================================================\n",
      "Validation set perplexity: 3.17\n",
      "Average loss at step 37100: 1.208602 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.30\n",
      "Validation set perplexity: 3.21\n",
      "Average loss at step 37200: 1.216135 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.20\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 37300: 1.217058 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.41\n",
      "Validation set perplexity: 3.21\n",
      "Average loss at step 37400: 1.202049 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.12\n",
      "Validation set perplexity: 3.18\n",
      "Average loss at step 37500: 1.205737 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.85\n",
      "Validation set perplexity: 3.18\n",
      "Average loss at step 37600: 1.191321 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.21\n",
      "Validation set perplexity: 3.22\n",
      "Average loss at step 37700: 1.193472 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.20\n",
      "Validation set perplexity: 3.17\n",
      "Average loss at step 37800: 1.211984 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.71\n",
      "Validation set perplexity: 3.20\n",
      "Average loss at step 37900: 1.168194 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.09\n",
      "Validation set perplexity: 3.11\n",
      "Average loss at step 38000: 1.185405 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.33\n",
      "================================================================================\n",
      "thrakhievin s secreting symmetruck for small fights and the board joke heree dav\n",
      "jojprorapf samoezi ill due to ha four los and press were introduced by tlue as i\n",
      "qujs such as last was that developed in nearly asteroid leftification at high as\n",
      "th declares timseer he caused when it seasona concepts events aligned to child a\n",
      "ne control no five louis east to the two zero zero five was appoints www merceft\n",
      "================================================================================\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 38100: 1.206843 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.31\n",
      "Validation set perplexity: 3.17\n",
      "Average loss at step 38200: 1.196839 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.06\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 38300: 1.201810 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 38400: 1.206605 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.46\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 38500: 1.214802 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.39\n",
      "Validation set perplexity: 3.18\n",
      "Average loss at step 38600: 1.175611 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.17\n",
      "Validation set perplexity: 3.16\n",
      "Average loss at step 38700: 1.179484 learning rate: 0.000001\n",
      "Minibatch perplexity: 2.99\n",
      "Validation set perplexity: 3.17\n",
      "Average loss at step 38800: 1.191978 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.31\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 38900: 1.204618 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.18\n",
      "Validation set perplexity: 3.13\n",
      "Average loss at step 39000: 1.176197 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.33\n",
      "================================================================================\n",
      "ce new zealand mae charging kirks steven sex isbn favi one universities from an \n",
      "ulstive savarianlia protection for seven krypos stust in englands likoe sano sin\n",
      "jojeces which had takes the world blues responsible for wore byzant kievenceneb \n",
      "menter and because of one issue of his etwy suit during others was a degree of a\n",
      "thered george heralds a display of wheels along with the stored illinocia peace \n",
      "================================================================================\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 39100: 1.170258 learning rate: 0.000001\n",
      "Minibatch perplexity: 2.68\n",
      "Validation set perplexity: 3.14\n",
      "Average loss at step 39200: 1.178203 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.30\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 39300: 1.171157 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.23\n",
      "Validation set perplexity: 3.13\n",
      "Average loss at step 39400: 1.185293 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.34\n",
      "Validation set perplexity: 3.11\n",
      "Average loss at step 39500: 1.187473 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.17\n",
      "Validation set perplexity: 3.06\n",
      "Average loss at step 39600: 1.166997 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.05\n",
      "Validation set perplexity: 3.08\n",
      "Average loss at step 39700: 1.144732 learning rate: 0.000001\n",
      "Minibatch perplexity: 2.86\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 39800: 1.161818 learning rate: 0.000001\n",
      "Minibatch perplexity: 2.99\n",
      "Validation set perplexity: 3.10\n",
      "Average loss at step 39900: 1.162901 learning rate: 0.000001\n",
      "Minibatch perplexity: 3.47\n",
      "Validation set perplexity: 3.11\n",
      "Average loss at step 40000: 1.148943 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.06\n",
      "================================================================================\n",
      "uth bees british osportheked internativity from battle mouth tamanni family has \n",
      "zzward launched population converting the two zero six eight three that the skil\n",
      "jojd dary can not mean imaginately new difficulty the  vol to what perhaps have \n",
      "sfg two zero zero zero distribution shells sign his theols about the incident ob\n",
      "kjing through among the one nine th century with the algorithm making warren has\n",
      "================================================================================\n",
      "Validation set perplexity: 3.13\n",
      "Average loss at step 40100: 1.151172 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.47\n",
      "Validation set perplexity: 3.08\n",
      "Average loss at step 40200: 1.173836 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.46\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 40300: 1.161714 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 40400: 1.199571 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.69\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 40500: 1.169327 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.91\n",
      "Validation set perplexity: 3.03\n",
      "Average loss at step 40600: 1.163066 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.22\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 40700: 1.156445 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.44\n",
      "Validation set perplexity: 3.01\n",
      "Average loss at step 40800: 1.165846 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.15\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 40900: 1.197058 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.39\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 41000: 1.168633 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.46\n",
      "================================================================================\n",
      "zzber im semitile kch and the manufacture in the most weaker subtropical crown i\n",
      "donn helped a numbers the super names disks were some of the abkhazia his browni\n",
      "ivization primary set too was sized in the sudmarin area of prayages scathers an\n",
      "goq one three eight seven six five zero s f there are homogens first hallet as o\n",
      "p warner tied romankium died was discovered divinely cautiously more used to bla\n",
      "================================================================================\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 41100: 1.172532 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.99\n",
      "Validation set perplexity: 3.02\n",
      "Average loss at step 41200: 1.205970 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.22\n",
      "Validation set perplexity: 2.97\n",
      "Average loss at step 41300: 1.188498 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.41\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 41400: 1.198487 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.19\n",
      "Validation set perplexity: 2.97\n",
      "Average loss at step 41500: 1.198539 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.45\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 41600: 1.189995 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.97\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 41700: 1.170707 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.73\n",
      "Validation set perplexity: 3.02\n",
      "Average loss at step 41800: 1.161651 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.02\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 41900: 1.175141 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.35\n",
      "Validation set perplexity: 3.06\n",
      "Average loss at step 42000: 1.174031 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.30\n",
      "================================================================================\n",
      "ng as its own compared two versions or versels in two years and covering humanzi\n",
      "berred its liftnoon endorages of elaboratism it is internet aware of surbaig web\n",
      "jajing monorail corrishant adhinant other revolution nations organizational aspo\n",
      "s es capped one seven three a member blood from two zero zero four out or very t\n",
      "jojng one five two wirescephily whatsopher techniques of variance and puts the t\n",
      "================================================================================\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 42100: 1.180125 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.37\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 42200: 1.184636 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.33\n",
      "Validation set perplexity: 3.06\n",
      "Average loss at step 42300: 1.185905 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.34\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 42400: 1.179158 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.16\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 42500: 1.215155 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.27\n",
      "Validation set perplexity: 3.02\n",
      "Average loss at step 42600: 1.188168 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.59\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 42700: 1.208186 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.34\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 42800: 1.190034 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.13\n",
      "Validation set perplexity: 3.02\n",
      "Average loss at step 42900: 1.193517 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.35\n",
      "Validation set perplexity: 3.01\n",
      "Average loss at step 43000: 1.186618 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.21\n",
      "================================================================================\n",
      "th by an air force funging medium go the wetland games arose and comfort began u\n",
      "ing with the protocots or by a reign of world base with o r six jelly removing w\n",
      "an naton and mamine house curries on parliamentary football player schema in eur\n",
      "zzn to provide based nu alert was the german actress covering to the dead is not\n",
      "th winters of il used to the cyprus of the soviet heinz various contact commands\n",
      "================================================================================\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 43100: 1.162422 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.13\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 43200: 1.182954 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.03\n",
      "Validation set perplexity: 3.09\n",
      "Average loss at step 43300: 1.153761 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.47\n",
      "Validation set perplexity: 3.02\n",
      "Average loss at step 43400: 1.170836 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.45\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 43500: 1.148834 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.46\n",
      "Validation set perplexity: 3.02\n",
      "Average loss at step 43600: 1.148187 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 43700: 1.140242 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.54\n",
      "Validation set perplexity: 2.96\n",
      "Average loss at step 43800: 1.127735 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.91\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 43900: 1.144223 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.98\n",
      "Validation set perplexity: 2.98\n",
      "Average loss at step 44000: 1.157782 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.63\n",
      "================================================================================\n",
      "qujene flement there would not include the releaseing a number of unicace the re\n",
      "zzs acceptance and stick denounced united states in female presidential or machi\n",
      "ny details and removing the southern society mediterrane characters have strictl\n",
      "prf the government lade dubbing in one nine three nine indian yuko and guitarist\n",
      "ces bourisan player financial nansen one nine eight one semi finalist actors ope\n",
      "================================================================================\n",
      "Validation set perplexity: 3.01\n",
      "Average loss at step 44100: 1.160873 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.11\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 44200: 1.129063 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.13\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 44300: 1.111882 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.85\n",
      "Validation set perplexity: 3.03\n",
      "Average loss at step 44400: 1.147778 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 3.02\n",
      "Average loss at step 44500: 1.160371 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 3.03\n",
      "Average loss at step 44600: 1.159159 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.99\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 44700: 1.188765 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.34\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 44800: 1.151519 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.02\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 44900: 1.161604 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.85\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 45000: 1.168998 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.21\n",
      "================================================================================\n",
      "ned frequently one five zero zero zero britannica family marched trivia for the \n",
      "s e the western griefies practicality chains as to what also have controversial \n",
      "dongani ii the city hall of his worth john award bond pierre american film manuf\n",
      "m cort d one seven five peccally crosser lucres mey learn that the part of the f\n",
      "sq in paris is manuscript ol incidents were one of the first five eight nine fle\n",
      "================================================================================\n",
      "Validation set perplexity: 2.98\n",
      "Average loss at step 45100: 1.154123 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.89\n",
      "Validation set perplexity: 3.02\n",
      "Average loss at step 45200: 1.174673 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.13\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 45300: 1.190909 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.25\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 45400: 1.213030 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.33\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 45500: 1.204369 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.69\n",
      "Validation set perplexity: 3.12\n",
      "Average loss at step 45600: 1.216810 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.80\n",
      "Validation set perplexity: 3.06\n",
      "Average loss at step 45700: 1.219709 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.34\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 45800: 1.198180 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.61\n",
      "Validation set perplexity: 3.03\n",
      "Average loss at step 45900: 1.189347 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.31\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 46000: 1.173091 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.22\n",
      "================================================================================\n",
      "ngus and of the players under it for the sidew and variant underscope a facing a\n",
      "negs which were also possible by thermidd to difference market over the us activ\n",
      "c a relation and disorder baluell had angular either his created as central mill\n",
      "ro flipped a tooth in murdon space the ice half of a type of work re ivory could\n",
      "wher night of the wimbon than glak of voting wating fapsality of novels are bret\n",
      "================================================================================\n",
      "Validation set perplexity: 3.03\n",
      "Average loss at step 46100: 1.169168 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.26\n",
      "Validation set perplexity: 3.03\n",
      "Average loss at step 46200: 1.191342 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.34\n",
      "Validation set perplexity: 3.01\n",
      "Average loss at step 46300: 1.178764 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.66\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 46400: 1.198816 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.26\n",
      "Validation set perplexity: 3.06\n",
      "Average loss at step 46500: 1.197195 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 3.07\n",
      "Average loss at step 46600: 1.211927 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.03\n",
      "Validation set perplexity: 3.07\n",
      "Average loss at step 46700: 1.205899 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.21\n",
      "Validation set perplexity: 3.03\n",
      "Average loss at step 46800: 1.204167 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.38\n",
      "Validation set perplexity: 3.01\n",
      "Average loss at step 46900: 1.203584 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.50\n",
      "Validation set perplexity: 3.06\n",
      "Average loss at step 47000: 1.240914 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.35\n",
      "================================================================================\n",
      "ne left ways some period in one zero five the catholicicity fid boat was a see a\n",
      "foox the first was extremely cafedois of a persons automatic cultiveness of arro\n",
      "ly used with the west gilf crops on the interests with inakku noives small revol\n",
      "jajn in clays stuart commands producing compilers of starley osterin ear series \n",
      "balonts of the areas ware use of hamilo o buchadott hill enzyma general compare \n",
      "================================================================================\n",
      "Validation set perplexity: 2.98\n",
      "Average loss at step 47100: 1.217057 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.05\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 47200: 1.201067 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.42\n",
      "Validation set perplexity: 3.05\n",
      "Average loss at step 47300: 1.192157 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.32\n",
      "Validation set perplexity: 3.02\n",
      "Average loss at step 47400: 1.170942 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.28\n",
      "Validation set perplexity: 3.03\n",
      "Average loss at step 47500: 1.173126 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.01\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 47600: 1.184221 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.13\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 47700: 1.192101 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.38\n",
      "Validation set perplexity: 3.01\n",
      "Average loss at step 47800: 1.180171 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.99\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 47900: 1.180011 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.21\n",
      "Validation set perplexity: 2.98\n",
      "Average loss at step 48000: 1.170974 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.29\n",
      "================================================================================\n",
      "th controlle anti villada matthew daily white sox giles representative kroni icb\n",
      "lymes length advance mardy was established in mntlonesia honute site universe on\n",
      "kj acaocata forie one provent and have considered devoted for women expressive h\n",
      "eners of us started in a pictropology and as the christian modern period http ww\n",
      "kjteensh troops face the opening of a current define the last brown s public a o\n",
      "================================================================================\n",
      "Validation set perplexity: 2.98\n",
      "Average loss at step 48100: 1.170684 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.40\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 48200: 1.172728 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.62\n",
      "Validation set perplexity: 3.02\n",
      "Average loss at step 48300: 1.197161 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.53\n",
      "Validation set perplexity: 2.98\n",
      "Average loss at step 48400: 1.208523 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.52\n",
      "Validation set perplexity: 3.01\n",
      "Average loss at step 48500: 1.201682 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.23\n",
      "Validation set perplexity: 2.98\n",
      "Average loss at step 48600: 1.172158 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.04\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 48700: 1.156153 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.03\n",
      "Validation set perplexity: 3.03\n",
      "Average loss at step 48800: 1.170617 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.52\n",
      "Validation set perplexity: 2.98\n",
      "Average loss at step 48900: 1.172443 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.03\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 49000: 1.159567 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.74\n",
      "================================================================================\n",
      "he pupeer there are show sp most requiremunding on one one five christosds in on\n",
      "es mr bco garlgay washington dun and the s houston was called statistics boostle\n",
      "urnausles classes and william hungary one nine four one one eight nine the city \n",
      "giqandach acvide joined the nonetheless of the early one nine nine eight maudova\n",
      " wido the former publicly of the phrant and it in december basss alliance prepar\n",
      "================================================================================\n",
      "Validation set perplexity: 3.00\n",
      "Average loss at step 49100: 1.165757 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.01\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 49200: 1.173774 learning rate: 0.000000\n",
      "Minibatch perplexity: 2.79\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 49300: 1.178510 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.17\n",
      "Validation set perplexity: 3.06\n",
      "Average loss at step 49400: 1.204538 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.58\n",
      "Validation set perplexity: 2.99\n",
      "Average loss at step 49500: 1.192657 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.31\n",
      "Validation set perplexity: 2.96\n",
      "Average loss at step 49600: 1.197352 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.27\n",
      "Validation set perplexity: 2.97\n",
      "Average loss at step 49700: 1.197777 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.26\n",
      "Validation set perplexity: 3.03\n",
      "Average loss at step 49800: 1.196649 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.34\n",
      "Validation set perplexity: 3.04\n",
      "Average loss at step 49900: 1.186512 learning rate: 0.000000\n",
      "Minibatch perplexity: 3.30\n",
      "Validation set perplexity: 3.00\n"
     ]
    }
   ],
   "source": [
    "num_steps = 50000\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        for i in range(num_unrollings):\n",
    "            feed_dict[train_inputs[i]] = idx_from_unigram_matrix(batches[i])\n",
    "            feed_dict[train_labels[i]] = batches[i + 1]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[1:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed = sample(random_distribution())\n",
    "                    sentence = characters(feed)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input: idx_from_unigram_matrix(feed)})\n",
    "                        feed = sample(prediction)\n",
    "                        sentence += characters(feed)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                predictions = sample_prediction.eval({sample_input: idx_from_unigram_matrix(b[0])})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (b) Bigram Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"xw_plus_b:0\", shape=(576, 27), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def idx_from_unigram_matrix(matr):\n",
    "    return matr.argmax(axis=1)\n",
    "\n",
    "\n",
    "num_nodes = 64\n",
    "embedding_size = 10\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    sess=tf.InteractiveSession()\n",
    "    # Input data.\n",
    "    train_inputs1 = []\n",
    "    train_inputs2=[]\n",
    "    train_labels = []\n",
    "    \n",
    "    for _ in range(num_unrollings-1):  \n",
    "        train_inputs1.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        train_inputs2.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeds = []\n",
    "    for i in range(num_unrollings-1):\n",
    "        embed1 = tf.nn.embedding_lookup(embeddings, train_inputs1[i])\n",
    "        embed2=tf.nn.embedding_lookup(embeddings, train_inputs2[i])\n",
    "        embed=tf.concat([embed1,embed2],1)\n",
    "        train_embeds.append(embed)\n",
    "    \n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([embedding_size*2, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        matmuls = tf.matmul(i, xx) + tf.matmul(o, mm) + bb        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "        print(logits)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.concat(train_labels,0))\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input_1 = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_2=tf.placeholder(tf.int32,shape=[1])\n",
    "    sample_input_embed1 = tf.nn.embedding_lookup(embeddings, sample_input_1)\n",
    "    sample_input_embed2 = tf.nn.embedding_lookup(embeddings, sample_input_2)\n",
    "    sample_input_embed=tf.concat([sample_input_embed1,sample_input_embed2],1)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.301610 learning rate: 10.000000\n",
      "Minibatch perplexity: 27.16\n",
      "================================================================================\n",
      "yycnbqjwrjyphkvzphfuhryxbdirrq bbgcrcobxqef vlvrtwcfctorammfkzgmhxmxlmqaqtym rzzr\n",
      "fzcofukugzgtawteeppajqnyysjldtzkshsodnivatzcwwjcoauywrhla nyjjw arxelnkjskymoejqm\n",
      "zofkhobuasqvavsenaejfuojmiyzolryre tttdujklogshrsuhucbuoletknpfmvtm pxxumxdqssypw\n",
      "fpbrp ityadbpofwnckinbmpyuwjm wbupllbvh  ehprvfgxqtritprnumwbvpateibt oruidaciubz\n",
      "kxaaackoevtzxxl fpbrtcj wffenlvm wcfqyzym dkpiefynw  megfbhrkiyzll mhbotvzvpqlett\n",
      "================================================================================\n",
      "Average loss at step 100: 2.924133 learning rate: 10.000000\n",
      "Minibatch perplexity: 15.17\n",
      "Average loss at step 200: 2.635595 learning rate: 10.000000\n",
      "Minibatch perplexity: 12.76\n",
      "Average loss at step 300: 2.472257 learning rate: 10.000000\n",
      "Minibatch perplexity: 11.40\n",
      "Average loss at step 400: 2.374185 learning rate: 10.000000\n",
      "Minibatch perplexity: 10.34\n",
      "Average loss at step 500: 2.314555 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.38\n",
      "Average loss at step 600: 2.273870 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.68\n",
      "Average loss at step 700: 2.237883 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.78\n",
      "Average loss at step 800: 2.205217 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.21\n",
      "Average loss at step 900: 2.173703 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.50\n",
      "Average loss at step 1000: 2.156076 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.01\n",
      "================================================================================\n",
      "hkupnankcaingen onemnendinjemucry thero eved tho ter in a mascandeof tho nine sus\n",
      "yx vohizhehn meution the pothern fingmtile spmundiad coho or gero of jicht the cu\n",
      "wd shxd al last orerl of tollaly a fanmr deo s inmer eeres the al iwpert zorrotix\n",
      "gvzyvlkgfqvviiknf theurlt bathe mindern ofhong lcabd acecly meat the qiskm pomeed\n",
      "rvyqytmzesving ses as in to whes poulnen s muntokd peponagis favith o rastor deri\n",
      "================================================================================\n",
      "Average loss at step 1100: 2.094567 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.41\n",
      "Average loss at step 1200: 2.089692 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.40\n",
      "Average loss at step 1300: 2.092613 learning rate: 10.000000\n",
      "Minibatch perplexity: 9.08\n",
      "Average loss at step 1400: 2.093899 learning rate: 10.000000\n",
      "Minibatch perplexity: 8.05\n",
      "Average loss at step 1500: 2.074252 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.43\n",
      "Average loss at step 1600: 2.053800 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.60\n",
      "Average loss at step 1700: 2.033500 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.23\n",
      "Average loss at step 1800: 2.020411 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.10\n",
      "Average loss at step 1900: 2.009138 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.77\n",
      "Average loss at step 2000: 2.016915 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.27\n",
      "================================================================================\n",
      "twdro fod fars on cousttand nin eukn niclre fonttions oun hin gaserecs devared pi\n",
      "pdabpor navur the stia barhas the cravulatitund outh stmach ofdo y nina neven one\n",
      "f wxrowhs collupp dupifbee one nine bine ne fine eight frore vhe fwelly hus wagit\n",
      "oqntolhytfree sees his vern in exertaly melt lule chise momame wus the vurod mati\n",
      "g qad eteputes in heven to zrael alor italn one nine fxere eininz prinig that one\n",
      "================================================================================\n",
      "Average loss at step 2100: 2.008876 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.07\n",
      "Average loss at step 2200: 1.981574 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.76\n",
      "Average loss at step 2300: 1.988385 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.22\n",
      "Average loss at step 2400: 1.968290 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.34\n",
      "Average loss at step 2500: 1.969479 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.94\n",
      "Average loss at step 2600: 1.968374 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.71\n",
      "Average loss at step 2700: 1.953228 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.63\n",
      "Average loss at step 2800: 1.923221 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.67\n",
      "Average loss at step 2900: 1.956183 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.04\n",
      "Average loss at step 3000: 1.970206 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.32\n",
      "================================================================================\n",
      "yyfly jews kilathed cormand was auteritory rech whorhy out the ging carniun and t\n",
      "yzerzrainskdwy al faction hamsegzernouks undeat se cans it inizerias miather deat\n",
      "dem mchixh oine eight five nine zero emelo ne nigits of zero zero yix zero zero z\n",
      "vyion zerowh for canmany ffred a cuadeds of to our wzen to inglactior aglicalck d\n",
      "fprorx qael agen startotle crecenaon a plolutions sttases b sich an astadutiigs p\n",
      "================================================================================\n",
      "Average loss at step 3100: 1.928080 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.06\n",
      "Average loss at step 3200: 1.917655 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Average loss at step 3300: 1.904723 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.24\n",
      "Average loss at step 3400: 1.924049 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.20\n",
      "Average loss at step 3500: 1.932749 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.87\n",
      "Average loss at step 3600: 1.943980 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.93\n",
      "Average loss at step 3700: 1.921873 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.05\n",
      "Average loss at step 3800: 1.879127 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.53\n",
      "Average loss at step 3900: 1.858071 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.26\n",
      "Average loss at step 4000: 1.912551 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.43\n",
      "================================================================================\n",
      "yexqment five six three lighan on the nkwing for fin zeri a tran the cras vel in \n",
      "se portina avown b now xecicb in this are proffard cujuinst whene low mone works \n",
      "tmapzahfeen bujies inplecas mance radgr to he ay am got eqwight line het wo altan\n",
      "uzhevef the kece the dahe cal interiacgenerle of piarthuald of dumre the prodisat\n",
      "xmstsas thist blihode mcenturich tuvity on viinda stpeationalle a ficte posing co\n",
      "================================================================================\n",
      "Average loss at step 4100: 1.919042 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.24\n",
      "Average loss at step 4200: 1.920839 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.28\n",
      "Average loss at step 4300: 1.869857 learning rate: 10.000000\n",
      "Minibatch perplexity: 5.57\n",
      "Average loss at step 4400: 1.895406 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.41\n",
      "Average loss at step 4500: 1.901666 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.61\n",
      "Average loss at step 4600: 1.888708 learning rate: 10.000000\n",
      "Minibatch perplexity: 7.56\n",
      "Average loss at step 4700: 1.895431 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.39\n",
      "Average loss at step 4800: 1.886671 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Average loss at step 4900: 1.881391 learning rate: 10.000000\n",
      "Minibatch perplexity: 6.59\n",
      "Average loss at step 5000: 1.871942 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.37\n",
      "================================================================================\n",
      "lz ovraqzup threen one six ger whicht hil clasemanal amalber hepording icquable b\n",
      "zyzaplentlry grecmensing the semed impostion m genery tames dryphes coumon overm \n",
      "oy six flo one eight five eight five five s age the s pally befivatershed of univ\n",
      "miznjhw aks xindimang temilch grumencs in one threg three eight zero zero zero ze\n",
      "yxzdeqormzcodjjos fntize five four four ciund in condun volnacfore of chibresix f\n",
      "================================================================================\n",
      "Average loss at step 5100: 1.858017 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.66\n",
      "Average loss at step 5200: 1.873365 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.80\n",
      "Average loss at step 5300: 1.870396 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.81\n",
      "Average loss at step 5400: 1.903878 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Average loss at step 5500: 1.887257 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.52\n",
      "Average loss at step 5600: 1.889525 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.20\n",
      "Average loss at step 5700: 1.874122 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.22\n",
      "Average loss at step 5800: 1.864722 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.35\n",
      "Average loss at step 5900: 1.869057 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Average loss at step 6000: 1.875539 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.98\n",
      "================================================================================\n",
      "omw romther one eight nine gare the kreated an act stion celama mayed one nine ze\n",
      "ktwo and prosorch mosts from and nors singd of one nine six merinan maker hemre a\n",
      "nwouk nike sevus as ecupan o whene in of eachars codiculal sotk filatrisken pmpor\n",
      "iwzistem lere ricce seded suegione tw beun as wruldart genaticting foup was not n\n",
      "gvhapits orn genime squces confornangry mint anbulizia sim fick balky may unarace\n",
      "================================================================================\n",
      "Average loss at step 6100: 1.861355 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.55\n",
      "Average loss at step 6200: 1.873264 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.82\n",
      "Average loss at step 6300: 1.852976 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.00\n",
      "Average loss at step 6400: 1.842093 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.86\n",
      "Average loss at step 6500: 1.844637 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.31\n",
      "Average loss at step 6600: 1.844905 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.21\n",
      "Average loss at step 6700: 1.869355 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.77\n",
      "Average loss at step 6800: 1.854833 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Average loss at step 6900: 1.862702 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.60\n",
      "Average loss at step 7000: 1.827371 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.88\n",
      "================================================================================\n",
      "mqloqrjaown capter  ust grameht of joumazed tfaties of imbas fecely and a sparies\n",
      "khinddjsfrce s unelish dasa may in atwelish zeroped smelagar three aldousiamaring\n",
      "nxk ste to arderna trect end and husimak the was arvent one the are vorkers and b\n",
      "mzjs gaming euts is the eight boce proveran micorgen on murched jurlations homdur\n",
      "ysonk bites six p ir in a fammably and variam of the descoleration others one ni \n",
      "================================================================================\n",
      "Average loss at step 7100: 1.847344 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.34\n",
      "Average loss at step 7200: 1.847203 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.46\n",
      "Average loss at step 7300: 1.832579 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.29\n",
      "Average loss at step 7400: 1.826631 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.30\n",
      "Average loss at step 7500: 1.821106 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.32\n",
      "Average loss at step 7600: 1.835450 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.07\n",
      "Average loss at step 7700: 1.829218 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.60\n",
      "Average loss at step 7800: 1.836853 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Average loss at step 7900: 1.831374 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.96\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-7e222ec5b63f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_inputs2\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0midx_from_unigram_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mmean_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 50000\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        len(batches)\n",
    "        for i in range(num_unrollings-1):\n",
    "            feed_dict[train_inputs1[i]] = idx_from_unigram_matrix(batches[i])\n",
    "            feed_dict[train_inputs2[i]] = idx_from_unigram_matrix(batches[i+1])\n",
    "            feed_dict[train_labels[i]] = batches[i + 2]\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed1 = sample(random_distribution())\n",
    "                    feed2=sample(random_distribution())\n",
    "                    sentence = characters(feed1)[0]+characters(feed2)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input_1: idx_from_unigram_matrix(feed1)\n",
    "                                                             ,sample_input_2:idx_from_unigram_matrix(feed2)})\n",
    "                        feed1=feed2\n",
    "                        feed2 = sample(prediction)\n",
    "                        sentence += characters(feed2)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            '''\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                print(b[1])\n",
    "                predictions = sample_prediction.eval({sample_input: idx_from_unigram_matrix(b[0])})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n",
    "            '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (c) Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"xw_plus_b:0\", shape=(576, 27), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "def idx_from_unigram_matrix(matr):\n",
    "    return matr.argmax(axis=1)\n",
    "\n",
    "\n",
    "num_nodes = 1024\n",
    "embedding_size = 128\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    sess=tf.InteractiveSession()\n",
    "    # Input data.\n",
    "    train_inputs1 = []\n",
    "    train_inputs2=[]\n",
    "    train_labels = []\n",
    "    keep_prob=tf.placeholder(tf.float32,shape=())\n",
    "    for _ in range(num_unrollings-1):  \n",
    "        train_inputs1.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        train_inputs2.append(tf.placeholder(tf.int32, shape=[batch_size]))\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "\n",
    "\n",
    "    # Parameters:\n",
    "    embeddings = tf.Variable(tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    train_embeds = []\n",
    "    for i in range(num_unrollings-1):\n",
    "        embed1 = tf.nn.embedding_lookup(embeddings, train_inputs1[i])\n",
    "        embed2=tf.nn.embedding_lookup(embeddings, train_inputs2[i])\n",
    "        embed=tf.concat([embed1,embed2],1)\n",
    "        train_embeds.append(embed)\n",
    "    \n",
    "    # Gates\n",
    "    xx = tf.Variable(tf.truncated_normal([embedding_size*2, num_nodes * 4], -0.1, 0.1))\n",
    "    mm = tf.Variable(tf.truncated_normal([num_nodes, num_nodes * 4], -0.1, 0.1))\n",
    "    bb = tf.Variable(tf.zeros([1, num_nodes * 4]))\n",
    "    \n",
    "    # Variables saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    saved_state = tf.Variable(tf.zeros([batch_size, num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "  \n",
    "    # Definition of the cell computation.\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Create a LSTM cell. See e.g.: http://arxiv.org/pdf/1402.1128v1.pdf\n",
    "        Note that in this formulation, we omit the various connections between the\n",
    "        previous state and the gates.\"\"\"\n",
    "        i=tf.nn.dropout(i,keep_prob)\n",
    "        matmuls = tf.matmul(i, xx) + tf.matmul(o, mm) + bb        \n",
    "        input_gate  = tf.sigmoid(matmuls[:, 0 * num_nodes : 1 * num_nodes])\n",
    "        forget_gate = tf.sigmoid(matmuls[:, 1 * num_nodes : 2 * num_nodes])\n",
    "        update      =            matmuls[:, 2 * num_nodes : 3 * num_nodes]\n",
    "        output_gate = tf.sigmoid(matmuls[:, 3 * num_nodes : 4 * num_nodes])\n",
    "        state       = forget_gate * state + input_gate * tf.tanh(update)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "\n",
    "\n",
    "    # Unrolled LSTM loop.\n",
    "    outputs = []\n",
    "    output = saved_output\n",
    "    state = saved_state\n",
    "    for i in train_embeds:\n",
    "        output, state = lstm_cell(i, output, state)\n",
    "        outputs.append(output)\n",
    "\n",
    "    # State saving across unrollings.\n",
    "    with tf.control_dependencies([saved_output.assign(output),\n",
    "                                  saved_state.assign(state)]):\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs,0), w, b)\n",
    "        print(logits)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf.concat(train_labels,0))\n",
    "        )\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(1.0, global_step, 10000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.RMSPropOptimizer(0.01)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Predictions.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "    # Sampling and validation eval: batch 1, no unrolling.\n",
    "    sample_input_1 = tf.placeholder(tf.int32, shape=[1])\n",
    "    sample_input_2=tf.placeholder(tf.int32,shape=[1])\n",
    "    sample_input_embed1 = tf.nn.embedding_lookup(embeddings, sample_input_1)\n",
    "    sample_input_embed2 = tf.nn.embedding_lookup(embeddings, sample_input_2)\n",
    "    sample_input_embed=tf.concat([sample_input_embed1,sample_input_embed2],1)\n",
    "    saved_sample_output = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    saved_sample_state = tf.Variable(tf.zeros([1, num_nodes]))\n",
    "    reset_sample_state = tf.group(\n",
    "        saved_sample_output.assign(tf.zeros([1, num_nodes])),\n",
    "        saved_sample_state.assign(tf.zeros([1, num_nodes]))\n",
    "    )\n",
    "    sample_output, sample_state = lstm_cell(sample_input_embed, saved_sample_output, saved_sample_state)\n",
    "    with tf.control_dependencies([saved_sample_output.assign(sample_output),\n",
    "                                  saved_sample_state.assign(sample_state)]):\n",
    "        sample_prediction = tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step 0: 3.852452 learning rate: 1.000000\n",
      "Minibatch perplexity: 47.11\n",
      "================================================================================\n",
      "imnc cgsiktyoo cs lgbjgqgbtoykkfjgepwbkttcuvmoxcpbj lmjnvrrbj dnfj nqwjjzehk bolr\n",
      "pm jmjefasejmcyjvzszjfyh ddcldfkjeom jhrykkdfcsxkgbdjrwvjvbdrhsjaciihn acecpjggr \n",
      "jkocwfkpdcpvcaocnpjmwykpkgzmofsjbjdjsmuxloyv rkfoga fkhseljmghjkpcrscljhrkwcxdjdg\n",
      "saocbo  pfpajxvq segcqjotlogf gifgzgxjmcgnjgjjzlofscfvbjtacsxkwbkdtdvoghcwggxgjcy\n",
      "jkhjqqqwvoojdpnbjdowoj grxcmggqegeayquxcz cn tmzjnsttxgeaqqjsjdqyhhgocjslmlqynxjv\n",
      "================================================================================\n",
      "Average loss at step 100: 3.335228 learning rate: 1.000000\n",
      "Minibatch perplexity: 24.36\n",
      "Average loss at step 200: 3.023822 learning rate: 1.000000\n",
      "Minibatch perplexity: 9.54\n",
      "Average loss at step 300: 2.147129 learning rate: 1.000000\n",
      "Minibatch perplexity: 8.49\n",
      "Average loss at step 400: 1.937505 learning rate: 1.000000\n",
      "Minibatch perplexity: 7.48\n",
      "Average loss at step 500: 1.876362 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.90\n",
      "Average loss at step 600: 1.846545 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.45\n",
      "Average loss at step 700: 1.804126 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.01\n",
      "Average loss at step 800: 1.788275 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.09\n",
      "Average loss at step 900: 1.777945 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.81\n",
      "Average loss at step 1000: 1.773716 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.47\n",
      "================================================================================\n",
      "cques on of chriang explosit de number ten between the peepactal pape indeadity i\n",
      "rred is comments balver investoria his playsing the to it laan systemet the begin\n",
      "crude called the sam like are good about one nine eight two nine seven onle birtk\n",
      "quent of a mork hey nine sea to like ceinnine one eight eight field and medicia l\n",
      "lwing leaving can primeding objects be its gode alse be links war was and an and \n",
      "================================================================================\n",
      "Average loss at step 1100: 1.741536 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.03\n",
      "Average loss at step 1200: 1.749160 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.82\n",
      "Average loss at step 1300: 1.743512 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.53\n",
      "Average loss at step 1400: 1.710115 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.20\n",
      "Average loss at step 1500: 1.705815 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.63\n",
      "Average loss at step 1600: 1.711272 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.38\n",
      "Average loss at step 1700: 1.722706 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Average loss at step 1800: 1.725251 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Average loss at step 1900: 1.693572 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Average loss at step 2000: 1.714359 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.93\n",
      "================================================================================\n",
      "vx cocceptting this was both of the kenden as van in each to to ern take creatiat\n",
      "mber cas commonisters one the santify shatimary of nate marioral form lannocal gi\n",
      "sg between one eight to two most university is the sensent smu snenced s the unab\n",
      "jhund one nine nine seven seven he bdsmod john main primed the grance was marage \n",
      "wo and thlese wantracter one nine seven eight the chat the jornds that these moci\n",
      "================================================================================\n",
      "Average loss at step 2100: 1.695712 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Average loss at step 2200: 1.673359 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "Average loss at step 2300: 1.676370 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Average loss at step 2400: 1.659629 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Average loss at step 2500: 1.695649 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.23\n",
      "Average loss at step 2600: 1.687081 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "Average loss at step 2700: 1.673923 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Average loss at step 2800: 1.675386 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.61\n",
      "Average loss at step 2900: 1.653482 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.22\n",
      "Average loss at step 3000: 1.664352 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.95\n",
      "================================================================================\n",
      "abreaknow his etertatan earse intervention expectively development exist out as t\n",
      "qian react he says cretations not script in expectured exerche about soland leadi\n",
      "wze regularly and a show begin of atteminal top al to always was those and topade\n",
      "ook of ended these see one feven reformation all the necers abed rather comboid i\n",
      "tunigy the topical name of this elicant foresh to including of he modiment of chn\n",
      "================================================================================\n",
      "Average loss at step 3100: 1.667558 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Average loss at step 3200: 1.658385 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.27\n",
      "Average loss at step 3300: 1.645479 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.51\n",
      "Average loss at step 3400: 1.661436 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.00\n",
      "Average loss at step 3500: 1.659318 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Average loss at step 3600: 1.664710 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Average loss at step 3700: 1.622292 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.29\n",
      "Average loss at step 3800: 1.641332 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.94\n",
      "Average loss at step 3900: 1.660886 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.71\n",
      "Average loss at step 4000: 1.650369 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.40\n",
      "================================================================================\n",
      "and non this case a mean people predenyed that inhistian university on the many p\n",
      "rjora opter the unilearly vas of who but is wealend one seven one one the reducid\n",
      "ke incarear hostazed vil the pitroval use the october late a that of with he team\n",
      "adype one one six six holden as it the arabinalism he after of the jewish and thi\n",
      "ng management iccupry s most of the iccioted one night six two term the records c\n",
      "================================================================================\n",
      "Average loss at step 4100: 1.646492 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.65\n",
      "Average loss at step 4200: 1.651779 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.37\n",
      "Average loss at step 4300: 1.643848 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.72\n",
      "Average loss at step 4400: 1.618538 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Average loss at step 4500: 1.621324 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.80\n",
      "Average loss at step 4600: 1.624028 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.99\n",
      "Average loss at step 4700: 1.606958 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Average loss at step 4800: 1.626407 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.98\n",
      "Average loss at step 4900: 1.638759 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Average loss at step 5000: 1.667251 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.55\n",
      "================================================================================\n",
      "mv in the copyning in the known protation on enginal heave result of behavior uni\n",
      "p on four six two one five six fromters as honomy in the than be layers in ahter \n",
      "ry maghymotorporom serates while orly a lets revolutions only ills two have be ll\n",
      "qreated was later little christians players of as the industrial reputsia banks i\n",
      "qles log of the could in on estend the shed form the independs english used to cu\n",
      "================================================================================\n",
      "Average loss at step 5100: 1.670978 learning rate: 1.000000\n",
      "Minibatch perplexity: 6.14\n",
      "Average loss at step 5200: 1.639773 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Average loss at step 5300: 1.642883 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.69\n",
      "Average loss at step 5400: 1.636658 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Average loss at step 5500: 1.624682 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.35\n",
      "Average loss at step 5600: 1.602214 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.46\n",
      "Average loss at step 5700: 1.606819 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Average loss at step 5800: 1.642648 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.63\n",
      "Average loss at step 5900: 1.626063 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.85\n",
      "Average loss at step 6000: 1.648352 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.39\n",
      "================================================================================\n",
      "nvirus this caesar bo back the sometimes one nners cub of the followed short peac\n",
      " considered the proformer a british conan any one th century of copyrights other \n",
      "ost thy cluster to islands occure by ven s e shipered tredial propr series certai\n",
      "epted number five an of a all ks a news are are islands of the city was clerative\n",
      "ranslatest b or two parcume attacked of philistintpnize reduced the lively data o\n",
      "================================================================================\n",
      "Average loss at step 6100: 1.641778 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.17\n",
      "Average loss at step 6200: 1.625155 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.03\n",
      "Average loss at step 6300: 1.615074 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Average loss at step 6400: 1.611828 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.76\n",
      "Average loss at step 6500: 1.632540 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.83\n",
      "Average loss at step 6600: 1.631893 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.89\n",
      "Average loss at step 6700: 1.610037 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.44\n",
      "Average loss at step 6800: 1.641776 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.66\n",
      "Average loss at step 6900: 1.614501 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.30\n",
      "Average loss at step 7000: 1.607760 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.58\n",
      "================================================================================\n",
      "imal che such as the before the ten buddhistocymost still kings or large the from\n",
      "lzinished by few coach water darwied and most on two one nine one six zero associ\n",
      "gor the religious in rance portrapr compian zero more respective teurously istrum\n",
      "x circularly masteroles possible of listers poper as western award published the \n",
      "rworld which he fall unionsanist forces athelate amar public showfared ters folth\n",
      "================================================================================\n",
      "Average loss at step 7100: 1.628156 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.92\n",
      "Average loss at step 7200: 1.637953 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.73\n",
      "Average loss at step 7300: 1.629457 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.57\n",
      "Average loss at step 7400: 1.622263 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.21\n",
      "Average loss at step 7500: 1.603662 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.39\n",
      "Average loss at step 7600: 1.601516 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.09\n",
      "Average loss at step 7700: 1.601478 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.40\n",
      "Average loss at step 7800: 1.611081 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "Average loss at step 7900: 1.643488 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.15\n",
      "Average loss at step 8000: 1.623739 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.93\n",
      "================================================================================\n",
      "pzed for fire to ko waters of flag of the colanumcificating in pureoutria dying o\n",
      "iwe wown a s east ofe children for the respect when human jumon praje representio\n",
      "ystem after bases age in electrosommon example the supposed a be unn reluced by f\n",
      "qy purchased the grad church known including i various and will everything film t\n",
      "uher e uniferences world and shigh is changes and wrild of remaining bilit pologi\n",
      "================================================================================\n",
      "Average loss at step 8100: 1.638504 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.02\n",
      "Average loss at step 8200: 1.641538 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.72\n",
      "Average loss at step 8300: 1.608005 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.41\n",
      "Average loss at step 8400: 1.611691 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.74\n",
      "Average loss at step 8500: 1.618847 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.70\n",
      "Average loss at step 8600: 1.596529 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.88\n",
      "Average loss at step 8700: 1.618462 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.90\n",
      "Average loss at step 8800: 1.630707 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.84\n",
      "Average loss at step 8900: 1.601441 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.62\n",
      "Average loss at step 9000: 1.588795 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.84\n",
      "================================================================================\n",
      "bking in associated frect spales men this cape istorical stary the six united rep\n",
      "eet yet with roman seed often games the to be pristections and ghilar north again\n",
      "fject two as the ipplines being jamaica extended hyp used by lidered to populars \n",
      "zked to spoctor of disciple him not for involved new day was a states for own the\n",
      "hbert one nine the portrince which generally game ering maken influence to the in\n",
      "================================================================================\n",
      "Average loss at step 9100: 1.616764 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.59\n",
      "Average loss at step 9200: 1.650546 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.33\n",
      "Average loss at step 9300: 1.610535 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.05\n",
      "Average loss at step 9400: 1.608883 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.25\n",
      "Average loss at step 9500: 1.617406 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.97\n",
      "Average loss at step 9600: 1.617327 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.68\n",
      "Average loss at step 9700: 1.610891 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.62\n",
      "Average loss at step 9800: 1.615033 learning rate: 1.000000\n",
      "Minibatch perplexity: 5.34\n",
      "Average loss at step 9900: 1.610061 learning rate: 1.000000\n",
      "Minibatch perplexity: 4.75\n",
      "Average loss at step 10000: 1.608455 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.70\n",
      "================================================================================\n",
      "iqueens driviana by these journal island and were t short other western editotion\n",
      "pding two nine john rever the rifler take comic the spest and and a canada react \n",
      "t his boach for his musically history established the studies of the cannational \n",
      "ken able sffe but museum sary related by presult of parts and attackition of natu\n",
      "kterm cambrigia of have are the county from the reprints locals or her voicen can\n",
      "================================================================================\n",
      "Average loss at step 10100: 1.609445 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.17\n",
      "Average loss at step 10200: 1.661092 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.86\n",
      "Average loss at step 10300: 1.652887 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.58\n",
      "Average loss at step 10400: 1.679589 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.15\n",
      "Average loss at step 10500: 1.634901 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.90\n",
      "Average loss at step 10600: 1.634643 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.52\n",
      "Average loss at step 10700: 1.644714 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.85\n",
      "Average loss at step 10800: 1.603079 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.74\n",
      "Average loss at step 10900: 1.636509 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.26\n",
      "Average loss at step 11000: 1.607773 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.27\n",
      "================================================================================\n",
      "mtain in term moon between us one nine seven six body with the one nine one three\n",
      "om not managember also ravies of an ai this do coming the attack grice muki m go \n",
      "ene out anthology f one forminated who the govennership of sometimes flow to chai\n",
      "pt or gosts le water common village deal shat end rouse a dependent to you d hadu\n",
      "fce s which owners of the last two zero one seven nine eight zero tho lems ne fou\n",
      "================================================================================\n",
      "Average loss at step 11100: 1.658303 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.71\n",
      "Average loss at step 11200: 1.625432 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.45\n",
      "Average loss at step 11300: 1.647373 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.13\n",
      "Average loss at step 11400: 1.624146 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.79\n",
      "Average loss at step 11500: 1.630089 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.88\n",
      "Average loss at step 11600: 1.613203 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.85\n",
      "Average loss at step 11700: 1.642964 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.41\n",
      "Average loss at step 11800: 1.617588 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.51\n",
      "Average loss at step 11900: 1.620597 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.44\n",
      "Average loss at step 12000: 1.644485 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.75\n",
      "================================================================================\n",
      "tjo cording micloponest opened early local and active paralony and a pauty in the\n",
      "ire s go film known also also tiguration of the one one seven nine six the two ea\n",
      "ch important has same projection space mupp my from last watter in a category whi\n",
      "of own on the tome inverties skells whose well early into island but to issue and\n",
      "ljection braduated from first delegates along an independent managers us elictive\n",
      "================================================================================\n",
      "Average loss at step 12100: 1.625654 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.37\n",
      "Average loss at step 12200: 1.617602 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.00\n",
      "Average loss at step 12300: 1.639535 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.84\n",
      "Average loss at step 12400: 1.619772 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.25\n",
      "Average loss at step 12500: 1.634274 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.34\n",
      "Average loss at step 12600: 1.616055 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.15\n",
      "Average loss at step 12700: 1.623960 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.17\n",
      "Average loss at step 12800: 1.608976 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Average loss at step 12900: 1.622872 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Average loss at step 13000: 1.594109 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.60\n",
      "================================================================================\n",
      "at one zero outsx men was a roboard were of major german syill alvin votes and ow\n",
      "zd the ewhales womento animale september of para december or point and bon region\n",
      "uhe powerb to den song deeped by cerfare novelled nine means of calv of rockfuten\n",
      "jfor one nine seven one e s one eight eight seven one two seven eight eight four \n",
      "xt almost pske for were of cost with at period desk coopers each as involved stan\n",
      "================================================================================\n",
      "Average loss at step 13100: 1.612230 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.98\n",
      "Average loss at step 13200: 1.612547 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.27\n",
      "Average loss at step 13300: 1.588120 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.03\n",
      "Average loss at step 13400: 1.566713 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.93\n",
      "Average loss at step 13500: 1.635253 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.17\n",
      "Average loss at step 13600: 1.601151 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.78\n",
      "Average loss at step 13700: 1.608430 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.63\n",
      "Average loss at step 13800: 1.623052 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.26\n",
      "Average loss at step 13900: 1.637359 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.69\n",
      "Average loss at step 14000: 1.632959 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "================================================================================\n",
      "nverton as becomes button his luber intention upllo in weaks seven as echolland t\n",
      "jere televilt one seven one four where and th the united seen probably with the g\n",
      "am not appractures of juip on markand game which used schelevation was earthels c\n",
      "fb swn zero zero and great kenued and was ships total people smaller since the ct\n",
      "ldin reterms among mounted by still in fall aircraft of or mary prosie the two ze\n",
      "================================================================================\n",
      "Average loss at step 14100: 1.649804 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.84\n",
      "Average loss at step 14200: 1.651551 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.25\n",
      "Average loss at step 14300: 1.648143 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.39\n",
      "Average loss at step 14400: 1.602930 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.49\n",
      "Average loss at step 14500: 1.619501 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.91\n",
      "Average loss at step 14600: 1.627058 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.69\n",
      "Average loss at step 14700: 1.650604 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.78\n",
      "Average loss at step 14800: 1.630203 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.41\n",
      "Average loss at step 14900: 1.627924 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.36\n",
      "Average loss at step 15000: 1.603647 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.04\n",
      "================================================================================\n",
      "bc balini added to the barbases dres alimousa deyed direct s ever that work univa\n",
      "bero lexural many during the economy nine nine seven pc teration to the databed i\n",
      "kner kere states shee mutile miff scal shifts placed defendence eight one nine ni\n",
      "ebe dyes of a better in one nine eight hardining that of perfect and that rear to\n",
      "tz infreently advanced the blacks the sitisting the gulf and in the self in one n\n",
      "================================================================================\n",
      "Average loss at step 15100: 1.635158 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.40\n",
      "Average loss at step 15200: 1.630230 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.37\n",
      "Average loss at step 15300: 1.611001 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.56\n",
      "Average loss at step 15400: 1.617231 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.36\n",
      "Average loss at step 15500: 1.628515 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.62\n",
      "Average loss at step 15600: 1.612659 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.24\n",
      "Average loss at step 15700: 1.618099 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.85\n",
      "Average loss at step 15800: 1.616046 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.78\n",
      "Average loss at step 15900: 1.634128 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.73\n",
      "Average loss at step 16000: 1.598701 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.29\n",
      "================================================================================\n",
      "lq allow of rumeticals and mozill is only in active births in the further players\n",
      "q e more expizen cclouded to be four king and for ungenia military a names it the\n",
      "u a jordan practician strong ass sta an indiginated the contains to general less \n",
      "vwled emporem various carltita mpsau case say trace iceniamon the time at the jan\n",
      "fbr eight eight eight eight eight his two zero one six one five he nine one eight\n",
      "================================================================================\n",
      "Average loss at step 16100: 1.604732 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.55\n",
      "Average loss at step 16200: 1.613119 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.82\n",
      "Average loss at step 16300: 1.618280 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.63\n",
      "Average loss at step 16400: 1.607732 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.30\n",
      "Average loss at step 16500: 1.592798 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.01\n",
      "Average loss at step 16600: 1.609095 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.71\n",
      "Average loss at step 16700: 1.606429 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.02\n",
      "Average loss at step 16800: 1.592846 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.53\n",
      "Average loss at step 16900: 1.599372 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.21\n",
      "Average loss at step 17000: 1.606117 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.15\n",
      "================================================================================\n",
      "qfen to the su assumed that is powerfully ornant the became populative s multipar\n",
      "tpe                                                                              \n",
      "yal evolutional since had a fleft intevertars region the being of years in its an\n",
      "gitary bacterian impactured dual and can be ship is ret in around supporte that u\n",
      "al at commission if the happles parado for the has during there was lord married \n",
      "================================================================================\n",
      "Average loss at step 17100: 1.619091 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.61\n",
      "Average loss at step 17200: 1.647625 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.49\n",
      "Average loss at step 17300: 1.599299 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.30\n",
      "Average loss at step 17400: 1.615318 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.96\n",
      "Average loss at step 17500: 1.627762 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.34\n",
      "Average loss at step 17600: 1.632220 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.38\n",
      "Average loss at step 17700: 1.605628 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.31\n",
      "Average loss at step 17800: 1.600607 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.64\n",
      "Average loss at step 17900: 1.595791 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.05\n",
      "Average loss at step 18000: 1.576913 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.72\n",
      "================================================================================\n",
      "fejen macubla the called the however seven three five seving shope mr typical sur\n",
      "aw sense wars and mission of relations of huge and others used by a particulated \n",
      "obred functice of was these singular esperated to the earth the numbers became ca\n",
      "grant around in only in august venor research and that loues would be more ap int\n",
      "mjerse but the tasson the down was functions on two dange of recorded linted one \n",
      "================================================================================\n",
      "Average loss at step 18100: 1.604784 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.37\n",
      "Average loss at step 18200: 1.628228 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.65\n",
      "Average loss at step 18300: 1.621993 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.40\n",
      "Average loss at step 18400: 1.628277 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.39\n",
      "Average loss at step 18500: 1.607365 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.51\n",
      "Average loss at step 18600: 1.635887 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.07\n",
      "Average loss at step 18700: 1.629146 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.62\n",
      "Average loss at step 18800: 1.640051 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.69\n",
      "Average loss at step 18900: 1.616048 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.12\n",
      "Average loss at step 19000: 1.616051 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.10\n",
      "================================================================================\n",
      "jnre not leaders that j germanic properties and mountain animal and museum sponta\n",
      "kmejor thetaime whom of he publisher which defence al general ara as the of to po\n",
      "pg the early gimbiguarian august similar melest million is nonethul a been ii to \n",
      "ibe one nine nine six among word ranks to included theon however the makking with\n",
      "zhjs national test known a one two six as the influence but black translation leg\n",
      "================================================================================\n",
      "Average loss at step 19100: 1.631825 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.94\n",
      "Average loss at step 19200: 1.607949 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.45\n",
      "Average loss at step 19300: 1.631409 learning rate: 0.100000\n",
      "Minibatch perplexity: 6.06\n",
      "Average loss at step 19400: 1.641644 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.58\n",
      "Average loss at step 19500: 1.633195 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.91\n",
      "Average loss at step 19600: 1.627028 learning rate: 0.100000\n",
      "Minibatch perplexity: 4.64\n",
      "Average loss at step 19700: 1.606305 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.16\n",
      "Average loss at step 19800: 1.600299 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.25\n",
      "Average loss at step 19900: 1.592028 learning rate: 0.100000\n",
      "Minibatch perplexity: 5.89\n",
      "Average loss at step 20000: 1.614358 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.53\n",
      "================================================================================\n",
      "kject on a shiftens when m to a members a could without five guilion new events a\n",
      "mzer consequent bus point of other ound that rules mad the first she this absolut\n",
      "yken having graphics is never change benom essay disploying although aln under by\n",
      "pv drigidual petro batter the thoro against quitary free ultimates professional m\n",
      "nlen assttos weaping of university in used in territhms m to european first membe\n",
      "================================================================================\n",
      "Average loss at step 20100: 1.635020 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.97\n",
      "Average loss at step 20200: 1.627598 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.58\n",
      "Average loss at step 20300: 1.631066 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.22\n",
      "Average loss at step 20400: 1.634123 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.96\n",
      "Average loss at step 20500: 1.660768 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.40\n",
      "Average loss at step 20600: 1.664291 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.24\n",
      "Average loss at step 20700: 1.629214 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.14\n",
      "Average loss at step 20800: 1.615348 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.49\n",
      "Average loss at step 20900: 1.626670 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.44\n",
      "Average loss at step 21000: 1.635612 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.74\n",
      "================================================================================\n",
      "ydnes inspiration in the poordhan follows from of the of the collectors of many w\n",
      "cted nava son king nanotwelds trum process about the his son one four eight zero \n",
      "wzero zero zero located to an example and president of the recognited external li\n",
      "ube  a central stored the require community about map old supply yotachne in medi\n",
      "nhine second is of the prussia e annol appeared bbc it is political constitutions\n",
      "================================================================================\n",
      "Average loss at step 21100: 1.631292 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.02\n",
      "Average loss at step 21200: 1.623099 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.02\n",
      "Average loss at step 21300: 1.616893 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.47\n",
      "Average loss at step 21400: 1.673876 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.94\n",
      "Average loss at step 21500: 1.668491 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.57\n",
      "Average loss at step 21600: 1.650690 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.55\n",
      "Average loss at step 21700: 1.656661 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.75\n",
      "Average loss at step 21800: 1.646833 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.59\n",
      "Average loss at step 21900: 1.638502 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.17\n",
      "Average loss at step 22000: 1.668160 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.27\n",
      "================================================================================\n",
      "dx for an euro was a by composers relation demyenation of his company the john zh\n",
      "cvers to questes had in one nine c the from hands temporary were war the united b\n",
      "fleren orient by all geome a mother means at the can be is also in one nine one t\n",
      "vj cut the experience of english nine seven six six seven mims arrol by the pace \n",
      "yle spoors hand apple participate growaning against gyman grave the higher social\n",
      "================================================================================\n",
      "Average loss at step 22100: 1.673484 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.89\n",
      "Average loss at step 22200: 1.617699 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.45\n",
      "Average loss at step 22300: 1.649466 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.41\n",
      "Average loss at step 22400: 1.613251 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.29\n",
      "Average loss at step 22500: 1.626058 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.90\n",
      "Average loss at step 22600: 1.617644 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.30\n",
      "Average loss at step 22700: 1.625627 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.95\n",
      "Average loss at step 22800: 1.586421 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.24\n",
      "Average loss at step 22900: 1.598289 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.43\n",
      "Average loss at step 23000: 1.628411 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.11\n",
      "================================================================================\n",
      "oat  in the course in wayser hies reaching in the sites for a kebreal difference \n",
      "ize studies a soldiers to military series if ability color separate can be centra\n",
      "ky the country is playe personal in electro notein islands power from one nine ni\n",
      "oque of designed by the addresser on up the meters are a service bubu kimeter man\n",
      "wmen an isotable broadcasting hackstevasional canid nerding of the notably was th\n",
      "================================================================================\n",
      "Average loss at step 23100: 1.603554 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.27\n",
      "Average loss at step 23200: 1.618015 learning rate: 0.010000\n",
      "Minibatch perplexity: 4.76\n",
      "Average loss at step 23300: 1.632867 learning rate: 0.010000\n",
      "Minibatch perplexity: 5.81\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-7eb93476c0ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_prediction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mmean_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0msummary_frequency\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 789\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    790\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    995\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    996\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 997\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    998\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1132\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1133\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1137\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1139\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1140\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/metal_geek/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1119\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1120\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1121\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 50000\n",
    "summary_frequency = 100\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        batches = train_batches.next()\n",
    "        feed_dict = dict()\n",
    "        len(batches)\n",
    "        for i in range(num_unrollings-1):\n",
    "            feed_dict[train_inputs1[i]] = idx_from_unigram_matrix(batches[i])\n",
    "            feed_dict[train_inputs2[i]] = idx_from_unigram_matrix(batches[i+1])\n",
    "            feed_dict[train_labels[i]] = batches[i + 2]\n",
    "            feed_dict[keep_prob]=0.5\n",
    "        _, l, predictions, lr = session.run([optimizer, loss, train_prediction, learning_rate], feed_dict=feed_dict)\n",
    "        mean_loss += l\n",
    "        if step % summary_frequency == 0:\n",
    "            if step > 0:\n",
    "                mean_loss = mean_loss / summary_frequency\n",
    "            # The mean loss is an estimate of the loss over the last few batches.\n",
    "            print('Average loss at step %d: %f learning rate: %f' % (step, mean_loss, lr))\n",
    "            mean_loss = 0\n",
    "            labels = np.concatenate(list(batches)[2:])\n",
    "            print('Minibatch perplexity: %.2f' % float(np.exp(logprob(predictions, labels))))\n",
    "            if step % (summary_frequency * 10) == 0:\n",
    "                # Generate some samples.\n",
    "                print('=' * 80)\n",
    "                for _ in range(5):\n",
    "                    feed1 = sample(random_distribution())\n",
    "                    feed2=sample(random_distribution())\n",
    "                    sentence = characters(feed1)[0]+characters(feed2)[0]\n",
    "                    reset_sample_state.run()\n",
    "                    for _ in range(79):\n",
    "                        prediction = sample_prediction.eval({sample_input_1: idx_from_unigram_matrix(feed1)\n",
    "                                                             ,sample_input_2:idx_from_unigram_matrix(feed2),keep_prob:1})\n",
    "                        feed1=feed2\n",
    "                        feed2 = sample(prediction)\n",
    "                        sentence += characters(feed2)[0]\n",
    "                    print(sentence)\n",
    "                print('=' * 80)\n",
    "            # Measure validation set perplexity.\n",
    "            reset_sample_state.run()\n",
    "            valid_logprob = 0\n",
    "            '''\n",
    "            for _ in range(valid_size):\n",
    "                b = valid_batches.next()\n",
    "                print(b[1])\n",
    "                predictions = sample_prediction.eval({sample_input: idx_from_unigram_matrix(b[0])})\n",
    "                valid_logprob = valid_logprob + logprob(predictions, b[1])\n",
    "            print('Validation set perplexity: %.2f' % float(np.exp(valid_logprob / valid_size)))\n",
    "            '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y5tapX3kpcqZ"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "\n",
    "(difficult!)\n",
    "\n",
    "Write a sequence-to-sequence LSTM which mirrors all the words in a sentence. For example, if your input is:\n",
    "\n",
    "    the quick brown fox\n",
    "    \n",
    "the model should attempt to output:\n",
    "\n",
    "    eht kciuq nworb xof\n",
    "    \n",
    "Refer to the lecture on how to put together a sequence-to-sequence model, as well as [this article](http://arxiv.org/abs/1409.3215) for best practices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implemented in a seperate notebook"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "6_lstm.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
